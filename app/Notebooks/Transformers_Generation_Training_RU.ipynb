{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Transformers Generation Training RU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Blks7HWN9Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MQRI-JmN9Ls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d0294c9f-b580-4b45-86a3-dc5a176095b8"
      },
      "source": [
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7DrdgrQB8ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_qikR-QOHcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d11c05c8-c083-4851-8a27-1ae2346f52a3"
      },
      "source": [
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        " \n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 159.3 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhsqSD6GRE2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b6b94544-4f7e-4685-d845-e9fad7d42621"
      },
      "source": [
        "! pip install awscli\n",
        "! aws s3 sync --no-sign-request s3://models.dobro.ai/gpt2/ru/unfreeze_all gpt2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: awscli in /usr/local/lib/python3.6/dist-packages (1.18.112)\n",
            "Requirement already satisfied: rsa<=4.5.0,>=3.1.2; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (4.5)\n",
            "Requirement already satisfied: botocore==1.17.35 in /usr/local/lib/python3.6/dist-packages (from awscli) (1.17.35)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.15.2)\n",
            "Requirement already satisfied: PyYAML<5.4,>=3.10; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (3.13)\n",
            "Requirement already satisfied: colorama<0.4.4,>=0.2.5; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (0.4.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.3.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=4.5.0,>=3.1.2; python_version != \"3.4\"->awscli) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.35->awscli) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.35->awscli) (2.8.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.35->awscli) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.17.35->awscli) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lICQFj-Fn4uZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b9a62036-021b-4ad3-c0f3-94e377a0999c"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-yi_yBX-O2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b46180b-9f28-4206-b2d1-ea783c74a010"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFlCwY4BZgcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7675bbaf-ddad-4a78-ef34-b04a3dcbfe26"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-rdldjzt7\n",
            "Created temporary directory: /tmp/pip-req-tracker-fhev891_\n",
            "Created requirements tracker '/tmp/pip-req-tracker-fhev891_'\n",
            "Created temporary directory: /tmp/pip-install-9io01qyy\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-6q45_fds\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-fhev891_'\n",
            "    Running setup.py (path:/tmp/pip-req-build-6q45_fds/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-6q45_fds/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-6q45_fds has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-fhev891_'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-tgxecy8f\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-tgxecy8f\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-6q45_fds/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-6q45_fds/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-tgxecy8f --python-tag cp36\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-6q45_fds/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-tgxecy8f/apex-0.1-cp36-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp36-none-any.whl size=192740 sha256=c52142cec217f47cf84f02b85d8f0b3a1223715554b5f33d78bc7da3c559020a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rdldjzt7/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-6q45_fds\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex-0.1.dist-info\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex-0.1.dist-info/\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex/\n",
            "      Successfully uninstalled apex-0.1\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-fhev891_'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-QowDzeSit5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fdd0591-8eca-429b-cdca-ecd303b3134f"
      },
      "source": [
        "! pip install youtokentome"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtokentome in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KoFOAl_n--x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "# from tqdm import tqdm as tqdm_base\n",
        "# def tqdm(*args, **kwargs):\n",
        "#     if hasattr(tqdm_base, '_instances'):\n",
        "#         for instance in list(tqdm_base._instances):\n",
        "#             tqdm_base._decr_instances(instance)\n",
        "#     return tqdm_base(*args, **kwargs)\n",
        "\n",
        "from transformers import (WEIGHTS_NAME, AdamW, \n",
        "                          # WarmupLinearSchedule,\n",
        "                                  BertConfig, BertForMaskedLM, BertTokenizer,\n",
        "                                  GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
        "                                  OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
        "                                  RobertaConfig, RobertaForMaskedLM, RobertaTokenizer,\n",
        "                                  DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whcB1oX_o5jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger.setLevel('INFO')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7LgWLPwoDgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
        "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
        "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-EysG53iHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dict2obj(d):\n",
        "  if isinstance(d, list):\n",
        "    d = [dict2obj(x) for x in d]\n",
        "  if not isinstance(d, dict):\n",
        "    return d\n",
        "  class C(object):\n",
        "    pass\n",
        "  o = C()\n",
        "  for k in d:\n",
        "    o.__dict__[k] = dict2obj(d[k])\n",
        "  return o\n",
        "\n",
        "BLOCK_SIZE = 256\n",
        "\n",
        "parser = {}\n",
        "\n",
        "parser['train_data_file'] = './oxxxymiron_lyrics_end_text.txt' #'Lyrics_Ð—ÐµÐ¼Ñ„Ð¸Ñ€Ð° (Zemfira).txt'\n",
        "parser['input_dir'] = './gpt2/m_checkpoint-3364613'\n",
        "parser['output_dir'] = './textgenmodels'\n",
        "\n",
        "parser['eval_data_file'] = './oxxxymiron_lyrics_end_text.txt' #'Lyrics_Ð—ÐµÐ¼Ñ„Ð¸Ñ€Ð° (Zemfira).txt'\n",
        "parser['model_type'] = 'gpt2' # bert\n",
        "parser['model_name_or_path'] = 'gpt2-medium' # 'bert-base-cased'\n",
        "parser['mlm'] = False \n",
        "parser['mlm_probability'] = False\n",
        "\n",
        "parser['config_name'] = \"\"\n",
        "parser['tokenizer_name'] = \"\"\n",
        "parser['cache_dir'] = \"\"\n",
        "parser['block_size'] = BLOCK_SIZE\n",
        "parser['do_train'] = True\n",
        "parser['do_eval'] = True\n",
        "parser['evaluate_during_training'] = True\n",
        "parser['do_lower_case'] = True\n",
        "\n",
        "parser['per_gpu_train_batch_size'] = 2\n",
        "parser['per_gpu_eval_batch_size'] = 2\n",
        "parser['gradient_accumulation_steps'] = 10\n",
        "parser['learning_rate'] = 0.001 # 5e-5\n",
        "parser['weight_decay'] = 0.0\n",
        "parser['adam_epsilon'] = 1e-8\n",
        "parser['max_grad_norm'] = 1.0\n",
        "parser['num_train_epochs'] = 5.0\n",
        "parser['max_steps'] = -1\n",
        "parser['warmup_steps'] = 100\n",
        "\n",
        "parser['logging_steps'] = 50\n",
        "parser['save_steps'] = 50\n",
        "parser['save_total_limit'] = None\n",
        "parser['eval_all_checkpoints'] = True\n",
        "parser['no_cuda'] = False\n",
        "parser['overwrite_output_dir'] = True\n",
        "parser['overwrite_cache'] = True\n",
        "parser['seed'] = 42\n",
        "\n",
        "parser['fp16'] = True\n",
        "parser['fp16_opt_level'] = 'O1'\n",
        "parser['local_rank'] = -1\n",
        "parser['server_ip'] = \"\"\n",
        "parser['server_port'] = \"\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c1oL47oVsY",
        "colab_type": "text"
      },
      "source": [
        "# Data loading\n",
        "https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdxejtgroZnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path='train', block_size=BLOCK_SIZE):\n",
        "        assert os.path.isfile(file_path)\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(directory, 'cached_lm_' + str(block_size) + '_' + filename)\n",
        "\n",
        "        if os.path.exists(cached_features_file):\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'rb') as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "            tokenized_text = tokenizer.encode(text)\n",
        "\n",
        "            # TODO FIX WARNINGS WHERE SPECIAL TOKENS AND GPT2 OUTPUT TOO MUCH\n",
        "            for i in range(0, len(tokenized_text)-block_size+1, block_size): # Truncate in block of block_size\n",
        "                if parser['model_type'] == 'gpt2':\n",
        "                    self.examples.append(tokenized_text[i:i+block_size])\n",
        "                else:\n",
        "                    self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
        "                \n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'wb') as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7c504apCGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    dataset = TextDataset(tokenizer, file_path=args.eval_data_file if evaluate else args.train_data_file, block_size=args.block_size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix, use_mtime=False):\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, '{}-*'.format(checkpoint_prefix)))\n",
        "    if len(glob_checkpoints) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    ordering_and_checkpoint_path = []\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match('.*{}-([0-9]+)'.format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def mask_tokens(inputs, tokenizer, args):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
        "    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    # scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps = -1)\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
        "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = 'checkpoint'\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, '{}-{}'.format(checkpoint_prefix, global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    torch.save(parser, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\n",
        "        \"perplexity\": perplexity,\n",
        "        'eval_loss': eval_loss\n",
        "    }\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1AhHCpyp1MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# args = parser.parse_args()\n",
        "args = dict2obj(parser)\n",
        "\n",
        "if args.model_type in [\"bert\", \"roberta\", \"distilbert\"] and not args.mlm:\n",
        "  raise ValueError(\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
        "                    \"flag (masked language modeling).\")\n",
        "if args.eval_data_file is None and args.do_eval:\n",
        "  raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "                    \"or remove the --do_eval argument.\")\n",
        "\n",
        "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "  raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAOzVvOVqTSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup distant debugging if needed\n",
        "if args.server_ip and args.server_port:\n",
        "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "    import ptvsd\n",
        "    print(\"Waiting for debugger attach\")\n",
        "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "    ptvsd.wait_for_attach()\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "args.device = device"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qx4FyooWSu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3ac3b51-191f-481d-deec-ec3e9e93f07b"
      },
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "# Set seed\n",
        "set_seed(args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:53:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tgHKPySRBZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "import os\n",
        "import youtokentome as yttm\n",
        "import hashlib\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "import shutil\n",
        "import regex as re\n",
        "from os.path import samefile"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRwvZZnYRB4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NEW_LINE = '<|n|>'\n",
        "\n",
        "class YTEncoder(PreTrainedTokenizer):\n",
        "    def_name = 'encoder.model'\n",
        "    def __init__(self, filename, *inputs, **kwargs):\n",
        "        super().__init__(*inputs, **kwargs)\n",
        "        #self.max_len_single_sentence = BLOCK_SIZE # no default special tokens - you can update this value if you add special tokens\n",
        "        #self.max_len_sentences_pair = BLOCK_SIZE # no default special tokens - you can update this value if you add special tokens\n",
        "\n",
        "        if os.path.isdir(filename): filename = os.path.join(filename, self.def_name)\n",
        "\n",
        "        self.bpe = yttm.BPE(filename)\n",
        "        self.hash = hashlib.sha512(open(filename, 'rb').read()).hexdigest()[:10]\n",
        "        self.filename = filename\n",
        "\n",
        "    def encode(self, text):\n",
        "        if text and text[0] != ' ': text = ' ' + text\n",
        "        text = re.sub(r'(?=[^ ])([\\W])([\\w])',r'\\g<1> \\g<2>',text)\n",
        "        text = text.replace('\\n', f' {NEW_LINE} ')\n",
        "\n",
        "        return self.bpe.encode([text], output_type=yttm.OutputType.ID)[0]\n",
        "\n",
        "\n",
        "    def decode(self, tokens): # I hate regexps\n",
        "        if not isinstance(tokens,list):\n",
        "            tokens = tokens.tolist()\n",
        "        result = self.bpe.decode(tokens)[0]\n",
        "        result = re.sub(r'( )?(<\\|n\\|>)( )?', r'\\n', result)\n",
        "        result = re.sub(r'([\\n(]) (\\w)',r'\\g<1>\\g<2>', result)\n",
        "        result = re.sub(r'(\\W)([Â«\"''\\n(]|^) (\\w)',r'\\g<1>\\g<2>\\g<3>', result)\n",
        "        result = re.sub(r'(\\w)- (\\w)',r'\\g<1>-\\g<2>', result)\n",
        "        return result\n",
        "\n",
        "    def tokenize(self, text, **kwargs):\n",
        "        return self.encode(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *inputs, **kwargs):\n",
        "        return cls(*inputs, **kwargs)\n",
        "\n",
        "    def add_special_tokens_single_sentence(self, token_ids):\n",
        "        return token_ids\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        src = self.filename\n",
        "        dst = os.path.join(save_directory, self.def_name)\n",
        "        if src != dst:\n",
        "            shutil.copyfile(src, dst)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcdDevtBWVus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "a553b62a-3bbd-4457-c19d-c6bd5d5152cb"
      },
      "source": [
        "# Load pretrained model and tokenizer\n",
        "if args.local_rank not in [-1, 0]:\n",
        "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "\n",
        "model = model_class.from_pretrained(args.input_dir)\n",
        "tokenizer = YTEncoder.from_pretrained(args.input_dir)\n",
        "model.to(args.device)\n",
        "\n",
        "if args.block_size <= 0:\n",
        "    args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "\n",
        "if args.local_rank == 0:\n",
        "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "logger.info(\"Training/evaluation parameters %s\", args)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:53:14 - INFO - transformers.configuration_utils -   loading configuration file ./gpt2/m_checkpoint-3364613/config.json\n",
            "08/05/2020 15:53:14 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"output_past\": true,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/05/2020 15:53:14 - INFO - transformers.modeling_utils -   loading weights file ./gpt2/m_checkpoint-3364613/pytorch_model.bin\n",
            "08/05/2020 15:53:28 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "08/05/2020 15:53:28 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at ./gpt2/m_checkpoint-3364613 and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "08/05/2020 15:53:39 - INFO - __main__ -   Training/evaluation parameters <__main__.dict2obj.<locals>.C object at 0x7fc43a333828>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv39Fsh-ZshR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size x vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None, xlm_lang=None, device='cuda'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            if is_xlnet: \n",
        "                # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
        "                # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
        "                input_ids = torch.cat((generated, torch.zeros((1, 1), dtype=torch.long, device=device)), dim=1)\n",
        "                perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "                perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
        "                target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "                target_mapping[0, 0, -1] = 1.0  # predict last token\n",
        "                inputs = {'input_ids': input_ids, 'perm_mask': perm_mask, 'target_mapping': target_mapping}\n",
        "\n",
        "            if is_xlm_mlm and xlm_mask_token:\n",
        "                # XLM MLM models are direct models (predict same token, not next token)\n",
        "                # => need one additional dummy token in the input (will be masked and guessed)\n",
        "                input_ids = torch.cat((generated, torch.full((1, 1), xlm_mask_token, dtype=torch.long, device=device)), dim=1)\n",
        "                inputs = {'input_ids': input_ids}\n",
        "\n",
        "            if xlm_lang is not None:\n",
        "                inputs[\"langs\"] = torch.tensor([xlm_lang] * inputs[\"input_ids\"].shape[1], device=device).view(1, -1)\n",
        "\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1rM-f2YZVtb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "7fd0c557-142d-4c90-84d8-35c0a8cd5b37"
      },
      "source": [
        "# CONTEXT_TEXT = 'Ð’ÑÑ‘ ÐµÑ‰Ñ‘ Ð±Ð°Ð±Ð»Ð° Ð½ÐµÑ‚, Ð²ÑÑ‘ ÐµÑ‰Ñ‘ Ñ Ð´Ð¾Ð»Ð³Ð°Ð¼Ð¸ ÐºÐ°Ð½Ð¸Ñ‚ÐµÐ»ÑŒ \\n \\\n",
        "# Ð’ÑÐµ ÐµÑ‰Ñ‘ Ð² Ð¿Ð¾Ð´Ð²Ð°Ð»Ðµ, Ð²ÑÑ‘ ÐµÑ‰Ñ‘ Parliament Ð½Ð° Ð±Ð¸Ñ‚Ðµ \\n \\\n",
        "# Ð˜ Ñ Ð²ÐµÑ€Ð½ÑƒÑÑŒ Ð½Ð° Ñ‚Ñ€ÐµÐº, Ñ‚Ð²Ð¾Ð¹ Ñ…ÑƒÐ¹ ÐºÐ°Ðº Ð¢ÑƒÐ»ÑƒÐ·-Ð›Ð¾Ñ‚Ñ€ÐµÐº \\n \\\n",
        "# Ð•ÑÐ»Ð¸ Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ñ€ÑÐ¿ Ð² Ð³Ñ€Ð¾Ð±Ñƒ ÑÑ‚Ð¾ Ð»ÐµÑ‚, Ñ‚Ð¾ Ñ ÐµÐ±Ñƒ ÑÐºÐµÐ»ÐµÑ‚ \\n \\\n",
        "# Ð˜ Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¸Ð» Ð°Ð»ÑŒÐ±Ð¾Ð¼ Ð½Ð° ÐºÐ¾ÑÑ‚ÑÑ… \\n \\\n",
        "# Ð­Ñ‚Ð¾Ð¼Ñƒ Ð½Ðµ Ð²Ð¸Ð´Ð½Ð¾ ÐºÐ¾Ð½Ñ†Ð°, ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ð¾Ð½ Ð³Ð¾Ð»Ñ‹Ð¹ Ñ‚Ð¾Ð»ÑÑ‚ÑÐº \\n \\\n",
        "# ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾ÑÐ¸Ñ‚ Ñ„Ð¸Ñ‚, ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ð¸ÑˆÐµÑ‚: Â«Ð”ÐµÐ½ÐµÐ³ Ð´Ð°Ð¼Â» \\n \\\n",
        "# Ð’Ð°Ñ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½, Ð½Ð¾ Ð¼Ð¾Ð¹ ÐºÑƒÐ¼Ð¸Ñ€ â€“ Ð“Ñ€Ð¸ÑˆÐ° ÐŸÐµÑ€ÐµÐ»ÑŒÐ¼Ð°Ð½ \\n'\n",
        "\n",
        "CONTEXT_TEXT = ''\n",
        "START_TEXT = 'Ð¯ Ð·Ð´ÐµÑÑŒ'\n",
        "CONTEXT_TEXT += START_TEXT\n",
        "\n",
        "context_tokens = tokenizer.encode(CONTEXT_TEXT)\n",
        "sampled = sample_sequence(model, \n",
        "                          100, \n",
        "                          context_tokens, \n",
        "                          temperature = 1.0,\n",
        "                          top_p=0.99\n",
        "                          )\n",
        "\n",
        "out = sampled[:, len(context_tokens):].tolist()\n",
        "text = ''.join([tokenizer.decode(o) for o in out])\n",
        "text = text[: text.find('<|endoftext|>')].split('\\n')\n",
        "\n",
        "print('-' * 20)\n",
        "\n",
        "for i, t in enumerate(text):\n",
        "    if i == 0:\n",
        "        print(START_TEXT + t)\n",
        "    else:\n",
        "        print(t)\n",
        "\n",
        "evaluate(args, model, tokenizer)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 33.19it/s]\n",
            "08/05/2020 15:53:42 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "08/05/2020 15:53:42 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "08/05/2020 15:53:42 - INFO - __main__ -     Num examples = 240\n",
            "08/05/2020 15:53:42 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating:   1%|          | 1/120 [00:00<00:15,  7.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Ð¯ Ð·Ð´ÐµÑÑŒÐ¿Ð¾Ñ‚Ð¾Ð¼Ñƒ, Ñ‡Ñ‚Ð¾ Ñ ÐµÐ³Ð¾ Ð±Ð¾ÑŽÑÑŒ. ÐÐµ Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾ Ð±Ð¾ÑŽÑÑŒ Ð‘Ð¾Ð½ÑƒÑÐ°, Ð”Ð¶ÐµÐºÐ°. Ð¡ÑƒÑ‚ÐºÐ¸ Ð½Ð°Ð·Ð°Ð´ Ñ ÑÐ¾Ð³Ð»Ð°ÑÐ¸Ð»Ð°ÑÑŒ Ð½Ð°Ð´ÐµÑ‚ÑŒ ÑÑ‚Ð¸ ÑÐ°Ð¿Ð¾Ð³Ð¸ Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ, Ñ‡Ñ‚Ð¾ Ð¿Ð¾ÑÐ»Ðµ Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ð² Ð¼Ñ‹ Ð¿Ð¾ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´Ð¸Ð¼ Ð² Ñ„Ð°Ð±Ñ€Ð¸ÐºÑƒ. Ð¯ ÑƒÐ¶Ðµ Ð¾Ð´ÐµÐ²Ð°ÑŽ Ð¸Ñ… Ð½Ð° ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÑƒÑŽ Ð¿Ñ€Ð¾Ñ‚ÑÐ¶ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚. ÐšÐ¾Ð½ÐµÑ‡Ð½Ð¾, Ð¼Ð¸ÑÑ‚ÐµÑ€ Ð‘Ð¾Ð½ÑƒÑ â€” ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÑ€ÑƒÑ‚Ð¾Ð¹, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÐµÐ³Ð¾ Ð¼ÐµÐ½Ñ Ð½Ð°Ð¹Ñ‚Ð¸. Ð¯ Ð±Ñ‹ Ð½Ðµ Ð½Ð°Ð´ÐµÐ»Ð° ÑÑ‚Ð¸ ÑÐ°Ð¿Ð¾Ð³Ð¸ Ð½Ð° ÑƒÐ»Ð¸Ñ†Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð¸ÐºÐ¾Ð³Ð¾ Ñ‚Ð°Ð¼ Ð½Ðµ ÑƒÐ²Ð¸Ð´ÐµÑ‚ÑŒ, Ñ Ð½Ðµ ÑÑ‚Ð¾ Ð¸Ð¼ÐµÐ»Ð° Ð² Ð²Ð¸Ð´Ñƒ, Ð½Ð¾ Ð¾Ð´Ð½Ð°Ð¶Ð´Ñ‹ Ñ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÐ»Ð° Ð² Ð³Ð»Ð°Ð·Ð¾Ðº â€” Ñ‚Ð°ÐºÐ¸Ñ… ÑÐ°Ð¿Ð¾Ð³ Ð·Ð´ÐµÑÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð²Ð°. (Â«... Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ð¸Ñ‡ÐµÐ³Ð¾?Â»\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:17<00:00,  6.88it/s]\n",
            "08/05/2020 15:54:00 - INFO - __main__ -   ***** Eval results  *****\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     eval_loss = 4.464891692002614\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     perplexity = tensor(86.9116)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 4.464891692002614, 'perplexity': tensor(86.9116)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anYKS2-8WZnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "511eba1d-eee1-4ba7-e108-f2a39bb30e2c"
      },
      "source": [
        "# Training\n",
        "if args.do_train:\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:54:00 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "08/05/2020 15:54:00 - INFO - __main__ -   ***** Running training *****\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Num examples = 240\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Num Epochs = 5\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 20\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Gradient Accumulation steps = 10\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Total optimization steps = 60\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/120 [00:00<00:31,  3.77it/s]\u001b[A\n",
            "Iteration:   2%|â–         | 2/120 [00:00<00:32,  3.64it/s]\u001b[A\n",
            "Iteration:   2%|â–Ž         | 3/120 [00:00<00:32,  3.56it/s]\u001b[A\n",
            "Iteration:   3%|â–Ž         | 4/120 [00:01<00:33,  3.50it/s]\u001b[A\n",
            "Iteration:   4%|â–         | 5/120 [00:01<00:32,  3.49it/s]\u001b[A\n",
            "Iteration:   5%|â–Œ         | 6/120 [00:01<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:   6%|â–Œ         | 7/120 [00:02<00:32,  3.48it/s]\u001b[A\n",
            "Iteration:   7%|â–‹         | 8/120 [00:02<00:32,  3.48it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 9/120 [00:02<00:32,  3.44it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "Iteration:   8%|â–Š         | 10/120 [00:03<00:36,  2.97it/s]\u001b[A\n",
            "Iteration:   9%|â–‰         | 11/120 [00:03<00:35,  3.05it/s]\u001b[A\n",
            "Iteration:  10%|â–ˆ         | 12/120 [00:03<00:34,  3.14it/s]\u001b[A\n",
            "Iteration:  11%|â–ˆ         | 13/120 [00:03<00:33,  3.21it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–        | 14/120 [00:04<00:32,  3.28it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–Ž        | 15/120 [00:04<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  13%|â–ˆâ–Ž        | 16/120 [00:04<00:31,  3.35it/s]\u001b[A\n",
            "Iteration:  14%|â–ˆâ–        | 17/120 [00:05<00:30,  3.39it/s]\u001b[A\n",
            "Iteration:  15%|â–ˆâ–Œ        | 18/120 [00:05<00:33,  3.05it/s]\u001b[A\n",
            "Iteration:  16%|â–ˆâ–Œ        | 19/120 [00:05<00:32,  3.15it/s]\u001b[A\n",
            "Iteration:  17%|â–ˆâ–‹        | 20/120 [00:06<00:35,  2.84it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 21/120 [00:06<00:33,  2.95it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 22/120 [00:06<00:31,  3.07it/s]\u001b[A\n",
            "Iteration:  19%|â–ˆâ–‰        | 23/120 [00:07<00:30,  3.16it/s]\u001b[A\n",
            "Iteration:  20%|â–ˆâ–ˆ        | 24/120 [00:07<00:29,  3.22it/s]\u001b[A\n",
            "Iteration:  21%|â–ˆâ–ˆ        | 25/120 [00:07<00:29,  3.25it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–       | 26/120 [00:08<00:28,  3.30it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–Ž       | 27/120 [00:08<00:27,  3.33it/s]\u001b[A\n",
            "Iteration:  23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:08<00:27,  3.36it/s]\u001b[A\n",
            "Iteration:  24%|â–ˆâ–ˆâ–       | 29/120 [00:08<00:27,  3.34it/s]\u001b[A\n",
            "Iteration:  25%|â–ˆâ–ˆâ–Œ       | 30/120 [00:09<00:30,  2.97it/s]\u001b[A\n",
            "Iteration:  26%|â–ˆâ–ˆâ–Œ       | 31/120 [00:09<00:28,  3.09it/s]\u001b[A\n",
            "Iteration:  27%|â–ˆâ–ˆâ–‹       | 32/120 [00:09<00:27,  3.16it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 33/120 [00:10<00:26,  3.23it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 34/120 [00:10<00:26,  3.25it/s]\u001b[A\n",
            "Iteration:  29%|â–ˆâ–ˆâ–‰       | 35/120 [00:10<00:25,  3.28it/s]\u001b[A\n",
            "Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:11<00:25,  3.32it/s]\u001b[A\n",
            "Iteration:  31%|â–ˆâ–ˆâ–ˆ       | 37/120 [00:11<00:24,  3.32it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–      | 38/120 [00:11<00:24,  3.34it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 39/120 [00:12<00:24,  3.36it/s]\u001b[A\n",
            "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:12<00:27,  2.95it/s]\u001b[A\n",
            "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–      | 41/120 [00:12<00:25,  3.05it/s]\u001b[A\n",
            "Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 42/120 [00:13<00:24,  3.12it/s]\u001b[A\n",
            "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 43/120 [00:13<00:24,  3.19it/s]\u001b[A\n",
            "Iteration:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:13<00:23,  3.23it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 45/120 [00:13<00:23,  3.25it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [00:14<00:22,  3.29it/s]\u001b[A\n",
            "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/120 [00:14<00:21,  3.33it/s]\u001b[A\n",
            "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:14<00:21,  3.34it/s]\u001b[A\n",
            "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 49/120 [00:15<00:21,  3.36it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/120 [00:15<00:23,  2.98it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 51/120 [00:15<00:22,  3.06it/s]\u001b[A\n",
            "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:16<00:21,  3.10it/s]\u001b[A\n",
            "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/120 [00:16<00:21,  3.18it/s]\u001b[A\n",
            "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 54/120 [00:16<00:20,  3.23it/s]\u001b[A\n",
            "Iteration:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [00:17<00:19,  3.26it/s]\u001b[A\n",
            "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:17<00:19,  3.30it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [00:17<00:19,  3.28it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/120 [00:17<00:18,  3.32it/s]\u001b[A\n",
            "Iteration:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/120 [00:18<00:18,  3.34it/s]\u001b[A\n",
            "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:18<00:20,  2.95it/s]\u001b[A\n",
            "Iteration:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 61/120 [00:18<00:19,  3.05it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [00:19<00:18,  3.14it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 63/120 [00:19<00:17,  3.21it/s]\u001b[A\n",
            "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:19<00:17,  3.26it/s]\u001b[A\n",
            "Iteration:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 65/120 [00:20<00:16,  3.28it/s]\u001b[A\n",
            "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 66/120 [00:20<00:16,  3.31it/s]\u001b[A\n",
            "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 67/120 [00:20<00:15,  3.32it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:21<00:15,  3.35it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/120 [00:21<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 70/120 [00:21<00:16,  2.97it/s]\u001b[A\n",
            "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/120 [00:22<00:15,  3.08it/s]\u001b[A\n",
            "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:22<00:15,  3.16it/s]\u001b[A\n",
            "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 73/120 [00:22<00:14,  3.21it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74/120 [00:22<00:14,  3.25it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 75/120 [00:23<00:13,  3.30it/s]\u001b[A\n",
            "Iteration:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:23<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 77/120 [00:23<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 78/120 [00:24<00:12,  3.39it/s]\u001b[A\n",
            "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 79/120 [00:24<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:24<00:13,  3.00it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/120 [00:25<00:12,  3.11it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 82/120 [00:25<00:11,  3.19it/s]\u001b[A\n",
            "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/120 [00:25<00:11,  3.24it/s]\u001b[A\n",
            "Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:26<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 85/120 [00:26<00:10,  3.32it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 86/120 [00:26<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 87/120 [00:26<00:09,  3.36it/s]\u001b[A\n",
            "Iteration:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:27<00:09,  3.36it/s]\u001b[A\n",
            "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 89/120 [00:27<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [00:27<00:10,  2.98it/s]\u001b[A\n",
            "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 91/120 [00:28<00:09,  3.09it/s]\u001b[A\n",
            "Iteration:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:28<00:08,  3.16it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/120 [00:28<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 94/120 [00:29<00:07,  3.29it/s]\u001b[A\n",
            "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 95/120 [00:29<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:29<00:07,  3.38it/s]\u001b[A\n",
            "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 97/120 [00:29<00:06,  3.37it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [00:30<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [00:30<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:31<00:06,  3.00it/s]\u001b[A\n",
            "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 101/120 [00:31<00:06,  3.10it/s]\u001b[A\n",
            "Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/120 [00:31<00:05,  3.18it/s]\u001b[A\n",
            "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 103/120 [00:31<00:05,  3.25it/s]\u001b[A\n",
            "Iteration:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:32<00:04,  3.30it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 105/120 [00:32<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [00:32<00:04,  3.39it/s]\u001b[A\n",
            "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 107/120 [00:33<00:03,  3.41it/s]\u001b[A\n",
            "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:33<00:03,  3.41it/s]\u001b[A\n",
            "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 109/120 [00:33<00:03,  3.43it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 110/120 [00:34<00:03,  3.02it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 111/120 [00:34<00:02,  3.11it/s]\u001b[A\n",
            "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:34<00:02,  3.18it/s]\u001b[A\n",
            "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/120 [00:34<00:02,  3.26it/s]\u001b[A\n",
            "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/120 [00:35<00:01,  3.31it/s]\u001b[A\n",
            "Iteration:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 115/120 [00:35<00:01,  3.36it/s]\u001b[A\n",
            "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:35<00:01,  3.38it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 117/120 [00:36<00:00,  3.39it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 118/120 [00:36<00:00,  3.41it/s]\u001b[A\n",
            "Iteration:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 119/120 [00:36<00:00,  3.41it/s]\u001b[A\n",
            "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:37<00:00,  3.23it/s]\n",
            "Epoch:  20%|â–ˆâ–ˆ        | 1/5 [00:37<02:28, 37.12s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.32it/s]\u001b[A\n",
            "Iteration:   2%|â–         | 2/120 [00:00<00:35,  3.35it/s]\u001b[A\n",
            "Iteration:   2%|â–Ž         | 3/120 [00:00<00:34,  3.39it/s]\u001b[A\n",
            "Iteration:   3%|â–Ž         | 4/120 [00:01<00:34,  3.40it/s]\u001b[A\n",
            "Iteration:   4%|â–         | 5/120 [00:01<00:33,  3.40it/s]\u001b[A\n",
            "Iteration:   5%|â–Œ         | 6/120 [00:01<00:33,  3.42it/s]\u001b[A\n",
            "Iteration:   6%|â–Œ         | 7/120 [00:02<00:33,  3.41it/s]\u001b[A\n",
            "Iteration:   7%|â–‹         | 8/120 [00:02<00:32,  3.44it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 9/120 [00:02<00:32,  3.44it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 10/120 [00:03<00:36,  3.04it/s]\u001b[A\n",
            "Iteration:   9%|â–‰         | 11/120 [00:03<00:35,  3.10it/s]\u001b[A\n",
            "Iteration:  10%|â–ˆ         | 12/120 [00:03<00:33,  3.20it/s]\u001b[A\n",
            "Iteration:  11%|â–ˆ         | 13/120 [00:03<00:32,  3.27it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–        | 14/120 [00:04<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–Ž        | 15/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  13%|â–ˆâ–Ž        | 16/120 [00:04<00:30,  3.38it/s]\u001b[A\n",
            "Iteration:  14%|â–ˆâ–        | 17/120 [00:05<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  15%|â–ˆâ–Œ        | 18/120 [00:05<00:29,  3.41it/s]\u001b[A\n",
            "Iteration:  16%|â–ˆâ–Œ        | 19/120 [00:05<00:29,  3.44it/s]\u001b[A\n",
            "Iteration:  17%|â–ˆâ–‹        | 20/120 [00:06<00:33,  3.02it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 21/120 [00:06<00:31,  3.13it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 22/120 [00:06<00:30,  3.23it/s]\u001b[A\n",
            "Iteration:  19%|â–ˆâ–‰        | 23/120 [00:06<00:29,  3.30it/s]\u001b[A\n",
            "Iteration:  20%|â–ˆâ–ˆ        | 24/120 [00:07<00:28,  3.36it/s]\u001b[A\n",
            "Iteration:  21%|â–ˆâ–ˆ        | 25/120 [00:07<00:27,  3.40it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–       | 26/120 [00:07<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–Ž       | 27/120 [00:08<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:08<00:26,  3.43it/s]\u001b[A\n",
            "Iteration:  24%|â–ˆâ–ˆâ–       | 29/120 [00:08<00:26,  3.45it/s]\u001b[A\n",
            "Iteration:  25%|â–ˆâ–ˆâ–Œ       | 30/120 [00:09<00:29,  3.04it/s]\u001b[A\n",
            "Iteration:  26%|â–ˆâ–ˆâ–Œ       | 31/120 [00:09<00:28,  3.13it/s]\u001b[A\n",
            "Iteration:  27%|â–ˆâ–ˆâ–‹       | 32/120 [00:09<00:27,  3.22it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 33/120 [00:09<00:26,  3.31it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 34/120 [00:10<00:25,  3.37it/s]\u001b[A\n",
            "Iteration:  29%|â–ˆâ–ˆâ–‰       | 35/120 [00:10<00:25,  3.38it/s]\u001b[A\n",
            "Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:10<00:24,  3.40it/s]\u001b[A\n",
            "Iteration:  31%|â–ˆâ–ˆâ–ˆ       | 37/120 [00:11<00:24,  3.43it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–      | 38/120 [00:11<00:23,  3.44it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 39/120 [00:11<00:23,  3.46it/s]\u001b[A\n",
            "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:12<00:25,  3.09it/s]\u001b[A\n",
            "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–      | 41/120 [00:12<00:24,  3.19it/s]\u001b[A\n",
            "Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 42/120 [00:12<00:23,  3.26it/s]\u001b[A\n",
            "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 43/120 [00:12<00:23,  3.32it/s]\u001b[A\n",
            "Iteration:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:13<00:22,  3.36it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 45/120 [00:13<00:22,  3.41it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [00:13<00:21,  3.43it/s]\u001b[A\n",
            "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/120 [00:14<00:21,  3.43it/s]\u001b[A\n",
            "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:14<00:20,  3.44it/s]\u001b[A\n",
            "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 49/120 [00:14<00:20,  3.47it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/120 [00:15<00:23,  3.03it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 51/120 [00:15<00:21,  3.14it/s]\u001b[A\n",
            "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:15<00:21,  3.24it/s]\u001b[A\n",
            "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/120 [00:15<00:20,  3.31it/s]\u001b[A\n",
            "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 54/120 [00:16<00:19,  3.34it/s]\u001b[A\n",
            "Iteration:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [00:16<00:19,  3.38it/s]\u001b[A\n",
            "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:16<00:18,  3.40it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [00:17<00:18,  3.43it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/120 [00:17<00:18,  3.42it/s]\u001b[A\n",
            "Iteration:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/120 [00:17<00:17,  3.43it/s]\u001b[A\n",
            "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:18<00:19,  3.06it/s]\u001b[A\n",
            "Iteration:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 61/120 [00:18<00:18,  3.17it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [00:18<00:17,  3.25it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 63/120 [00:19<00:17,  3.32it/s]\u001b[A\n",
            "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:19<00:16,  3.37it/s]\u001b[A\n",
            "Iteration:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 65/120 [00:19<00:16,  3.39it/s]\u001b[A\n",
            "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 66/120 [00:19<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 67/120 [00:20<00:15,  3.44it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:20<00:15,  3.45it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/120 [00:20<00:14,  3.45it/s]\u001b[A\n",
            "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 70/120 [00:21<00:16,  3.02it/s]\u001b[A\n",
            "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/120 [00:21<00:15,  3.13it/s]\u001b[A\n",
            "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:21<00:14,  3.22it/s]\u001b[A\n",
            "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 73/120 [00:22<00:14,  3.29it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74/120 [00:22<00:13,  3.34it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 75/120 [00:22<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:22<00:12,  3.42it/s]\u001b[A\n",
            "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 77/120 [00:23<00:12,  3.45it/s]\u001b[A\n",
            "Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 78/120 [00:23<00:12,  3.45it/s]\u001b[A\n",
            "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 79/120 [00:23<00:11,  3.46it/s]\u001b[A\n",
            "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:24<00:13,  3.06it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/120 [00:24<00:12,  3.13it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 82/120 [00:24<00:11,  3.22it/s]\u001b[A\n",
            "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/120 [00:25<00:11,  3.31it/s]\u001b[A\n",
            "Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:25<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 85/120 [00:25<00:10,  3.40it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 86/120 [00:25<00:09,  3.44it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 87/120 [00:26<00:09,  3.45it/s]\u001b[A\n",
            "Iteration:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:26<00:09,  3.47it/s]\u001b[A\n",
            "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 89/120 [00:26<00:08,  3.45it/s]\u001b[A\n",
            "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [00:27<00:09,  3.04it/s]\u001b[A\n",
            "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 91/120 [00:27<00:09,  3.16it/s]\u001b[A\n",
            "Iteration:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:27<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/120 [00:28<00:08,  3.30it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 94/120 [00:28<00:07,  3.36it/s]\u001b[A\n",
            "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 95/120 [00:28<00:07,  3.39it/s]\u001b[A\n",
            "Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:28<00:07,  3.42it/s]\u001b[A\n",
            "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 97/120 [00:29<00:06,  3.43it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [00:29<00:06,  3.45it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [00:29<00:06,  3.47it/s]\u001b[A\n",
            "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:30<00:06,  3.08it/s]\u001b[A\n",
            "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 101/120 [00:30<00:05,  3.18it/s]\u001b[A\n",
            "Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/120 [00:30<00:05,  3.26it/s]\u001b[A\n",
            "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 103/120 [00:31<00:05,  3.32it/s]\u001b[A\n",
            "Iteration:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:31<00:04,  3.37it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 105/120 [00:31<00:04,  3.36it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [00:31<00:04,  3.40it/s]\u001b[A\n",
            "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 107/120 [00:32<00:03,  3.43it/s]\u001b[A\n",
            "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:32<00:03,  3.45it/s]\u001b[A\n",
            "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 109/120 [00:32<00:03,  3.46it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 110/120 [00:33<00:03,  3.05it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 111/120 [00:33<00:02,  3.14it/s]\u001b[A\n",
            "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:33<00:02,  3.22it/s]\u001b[A\n",
            "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/120 [00:34<00:02,  3.30it/s]\u001b[A\n",
            "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/120 [00:34<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 115/120 [00:34<00:01,  3.37it/s]\u001b[A\n",
            "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:34<00:01,  3.40it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 117/120 [00:35<00:00,  3.43it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 118/120 [00:35<00:00,  3.45it/s]\u001b[A\n",
            "Iteration:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 119/120 [00:35<00:00,  3.47it/s]\u001b[A\n",
            "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:36<00:00,  3.31it/s]\n",
            "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:13<01:50, 36.86s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:34,  3.44it/s]\u001b[A\n",
            "Iteration:   2%|â–         | 2/120 [00:00<00:34,  3.44it/s]\u001b[A\n",
            "Iteration:   2%|â–Ž         | 3/120 [00:00<00:33,  3.46it/s]\u001b[A\n",
            "Iteration:   3%|â–Ž         | 4/120 [00:01<00:33,  3.47it/s]\u001b[A\n",
            "Iteration:   4%|â–         | 5/120 [00:01<00:33,  3.48it/s]\u001b[A\n",
            "Iteration:   5%|â–Œ         | 6/120 [00:01<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:   6%|â–Œ         | 7/120 [00:02<00:32,  3.47it/s]\u001b[A\n",
            "Iteration:   7%|â–‹         | 8/120 [00:02<00:32,  3.48it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 9/120 [00:02<00:31,  3.48it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 10/120 [00:03<00:35,  3.06it/s]\u001b[A\n",
            "Iteration:   9%|â–‰         | 11/120 [00:03<00:34,  3.17it/s]\u001b[A\n",
            "Iteration:  10%|â–ˆ         | 12/120 [00:03<00:33,  3.24it/s]\u001b[A\n",
            "Iteration:  11%|â–ˆ         | 13/120 [00:03<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–        | 14/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–Ž        | 15/120 [00:04<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  13%|â–ˆâ–Ž        | 16/120 [00:04<00:30,  3.44it/s]\u001b[A\n",
            "Iteration:  14%|â–ˆâ–        | 17/120 [00:05<00:29,  3.46it/s]\u001b[A\n",
            "Iteration:  15%|â–ˆâ–Œ        | 18/120 [00:05<00:29,  3.46it/s]\u001b[A\n",
            "Iteration:  16%|â–ˆâ–Œ        | 19/120 [00:05<00:29,  3.42it/s]\u001b[A\n",
            "Iteration:  17%|â–ˆâ–‹        | 20/120 [00:06<00:33,  3.02it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 21/120 [00:06<00:31,  3.14it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 22/120 [00:06<00:30,  3.22it/s]\u001b[A\n",
            "Iteration:  19%|â–ˆâ–‰        | 23/120 [00:06<00:29,  3.28it/s]\u001b[A\n",
            "Iteration:  20%|â–ˆâ–ˆ        | 24/120 [00:07<00:28,  3.34it/s]\u001b[A\n",
            "Iteration:  21%|â–ˆâ–ˆ        | 25/120 [00:07<00:28,  3.38it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–       | 26/120 [00:07<00:27,  3.39it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–Ž       | 27/120 [00:08<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:08<00:26,  3.43it/s]\u001b[A\n",
            "Iteration:  24%|â–ˆâ–ˆâ–       | 29/120 [00:08<00:26,  3.43it/s]\u001b[A\n",
            "Iteration:  25%|â–ˆâ–ˆâ–Œ       | 30/120 [00:09<00:29,  3.06it/s]\u001b[A\n",
            "Iteration:  26%|â–ˆâ–ˆâ–Œ       | 31/120 [00:09<00:28,  3.16it/s]\u001b[A\n",
            "Iteration:  27%|â–ˆâ–ˆâ–‹       | 32/120 [00:09<00:27,  3.25it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 33/120 [00:09<00:26,  3.30it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 34/120 [00:10<00:25,  3.34it/s]\u001b[A\n",
            "Iteration:  29%|â–ˆâ–ˆâ–‰       | 35/120 [00:10<00:25,  3.37it/s]\u001b[A\n",
            "Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:10<00:24,  3.39it/s]\u001b[A\n",
            "Iteration:  31%|â–ˆâ–ˆâ–ˆ       | 37/120 [00:11<00:24,  3.40it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–      | 38/120 [00:11<00:24,  3.41it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 39/120 [00:11<00:23,  3.43it/s]\u001b[A\n",
            "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:12<00:26,  3.03it/s]\u001b[A\n",
            "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–      | 41/120 [00:12<00:25,  3.14it/s]\u001b[A\n",
            "Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 42/120 [00:12<00:24,  3.23it/s]\u001b[A\n",
            "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 43/120 [00:12<00:23,  3.29it/s]\u001b[A\n",
            "Iteration:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:13<00:22,  3.32it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 45/120 [00:13<00:22,  3.38it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [00:13<00:21,  3.40it/s]\u001b[A\n",
            "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/120 [00:14<00:21,  3.42it/s]\u001b[A\n",
            "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:14<00:20,  3.43it/s]\u001b[A\n",
            "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 49/120 [00:14<00:20,  3.44it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/120 [00:15<00:23,  3.03it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 51/120 [00:15<00:22,  3.12it/s]\u001b[A\n",
            "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:15<00:21,  3.20it/s]\u001b[A\n",
            "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/120 [00:15<00:20,  3.25it/s]\u001b[A\n",
            "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 54/120 [00:16<00:19,  3.31it/s]\u001b[A\n",
            "Iteration:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [00:16<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:16<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [00:17<00:18,  3.35it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/120 [00:17<00:17,  3.40it/s]\u001b[A\n",
            "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:18<00:22,  2.71it/s]\u001b[A\n",
            "Iteration:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 61/120 [00:18<00:20,  2.89it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [00:18<00:19,  3.03it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 63/120 [00:19<00:18,  3.12it/s]\u001b[A\n",
            "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:19<00:17,  3.22it/s]\u001b[A\n",
            "Iteration:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 65/120 [00:19<00:16,  3.29it/s]\u001b[A\n",
            "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 66/120 [00:20<00:16,  3.31it/s]\u001b[A\n",
            "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 67/120 [00:20<00:15,  3.33it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:20<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/120 [00:20<00:15,  3.39it/s]\u001b[A\n",
            "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 70/120 [00:21<00:16,  3.01it/s]\u001b[A\n",
            "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/120 [00:21<00:15,  3.13it/s]\u001b[A\n",
            "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:21<00:14,  3.20it/s]\u001b[A\n",
            "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 73/120 [00:22<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74/120 [00:22<00:13,  3.33it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 75/120 [00:22<00:13,  3.37it/s]\u001b[A\n",
            "Iteration:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:23<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 77/120 [00:23<00:12,  3.39it/s]\u001b[A\n",
            "Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 78/120 [00:23<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 79/120 [00:23<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:24<00:13,  3.03it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/120 [00:24<00:12,  3.12it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 82/120 [00:24<00:11,  3.20it/s]\u001b[A\n",
            "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/120 [00:25<00:11,  3.25it/s]\u001b[A\n",
            "Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:25<00:10,  3.29it/s]\u001b[A\n",
            "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 85/120 [00:25<00:10,  3.33it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 86/120 [00:26<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 87/120 [00:26<00:09,  3.38it/s]\u001b[A\n",
            "Iteration:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:26<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 89/120 [00:27<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [00:27<00:10,  2.97it/s]\u001b[A\n",
            "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 91/120 [00:27<00:09,  3.07it/s]\u001b[A\n",
            "Iteration:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:28<00:08,  3.17it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/120 [00:28<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 94/120 [00:28<00:07,  3.28it/s]\u001b[A\n",
            "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 95/120 [00:28<00:07,  3.32it/s]\u001b[A\n",
            "Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:29<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 97/120 [00:29<00:06,  3.35it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [00:29<00:06,  3.36it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [00:30<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:30<00:06,  2.97it/s]\u001b[A\n",
            "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 101/120 [00:30<00:06,  3.06it/s]\u001b[A\n",
            "Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/120 [00:31<00:05,  3.17it/s]\u001b[A\n",
            "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 103/120 [00:31<00:05,  3.22it/s]\u001b[A\n",
            "Iteration:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:31<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 105/120 [00:32<00:04,  3.31it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [00:32<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 107/120 [00:32<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:32<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 109/120 [00:33<00:03,  3.40it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 110/120 [00:33<00:03,  2.99it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 111/120 [00:33<00:02,  3.08it/s]\u001b[A\n",
            "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:34<00:02,  3.18it/s]\u001b[A\n",
            "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/120 [00:34<00:02,  3.24it/s]\u001b[A\n",
            "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/120 [00:34<00:01,  3.28it/s]\u001b[A\n",
            "Iteration:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 115/120 [00:35<00:01,  3.31it/s]\u001b[A\n",
            "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:35<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 117/120 [00:35<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 118/120 [00:35<00:00,  3.39it/s]\u001b[A\n",
            "Iteration:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 119/120 [00:36<00:00,  3.41it/s]\u001b[A\n",
            "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:36<00:00,  3.27it/s]\n",
            "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:50<01:13, 36.81s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.33it/s]\u001b[A\n",
            "Iteration:   2%|â–         | 2/120 [00:00<00:35,  3.37it/s]\u001b[A\n",
            "Iteration:   2%|â–Ž         | 3/120 [00:00<00:34,  3.39it/s]\u001b[A\n",
            "Iteration:   3%|â–Ž         | 4/120 [00:01<00:34,  3.39it/s]\u001b[A\n",
            "Iteration:   4%|â–         | 5/120 [00:01<00:33,  3.40it/s]\u001b[A\n",
            "Iteration:   5%|â–Œ         | 6/120 [00:01<00:33,  3.39it/s]\u001b[A\n",
            "Iteration:   6%|â–Œ         | 7/120 [00:02<00:33,  3.41it/s]\u001b[A\n",
            "Iteration:   7%|â–‹         | 8/120 [00:02<00:32,  3.43it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 9/120 [00:02<00:32,  3.44it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 10/120 [00:03<00:36,  3.03it/s]\u001b[A\n",
            "Iteration:   9%|â–‰         | 11/120 [00:03<00:34,  3.13it/s]\u001b[A\n",
            "Iteration:  10%|â–ˆ         | 12/120 [00:03<00:33,  3.22it/s]\u001b[A\n",
            "Iteration:  11%|â–ˆ         | 13/120 [00:03<00:32,  3.28it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–        | 14/120 [00:04<00:31,  3.33it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–Ž        | 15/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  13%|â–ˆâ–Ž        | 16/120 [00:04<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  14%|â–ˆâ–        | 17/120 [00:05<00:30,  3.41it/s]\u001b[A\n",
            "Iteration:  15%|â–ˆâ–Œ        | 18/120 [00:05<00:29,  3.43it/s]\u001b[A\n",
            "Iteration:  16%|â–ˆâ–Œ        | 19/120 [00:05<00:29,  3.44it/s]\u001b[A\n",
            "Iteration:  17%|â–ˆâ–‹        | 20/120 [00:06<00:33,  3.00it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 21/120 [00:06<00:31,  3.11it/s]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 22/120 [00:06<00:30,  3.20it/s]\u001b[A\n",
            "Iteration:  19%|â–ˆâ–‰        | 23/120 [00:06<00:29,  3.27it/s]\u001b[A\n",
            "Iteration:  20%|â–ˆâ–ˆ        | 24/120 [00:07<00:28,  3.31it/s]\u001b[A\n",
            "Iteration:  21%|â–ˆâ–ˆ        | 25/120 [00:07<00:28,  3.33it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–       | 26/120 [00:07<00:27,  3.37it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–Ž       | 27/120 [00:08<00:27,  3.40it/s]\u001b[A\n",
            "Iteration:  23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:08<00:27,  3.41it/s]\u001b[A\n",
            "Iteration:  24%|â–ˆâ–ˆâ–       | 29/120 [00:08<00:26,  3.42it/s]\u001b[A\n",
            "Iteration:  25%|â–ˆâ–ˆâ–Œ       | 30/120 [00:09<00:29,  3.02it/s]\u001b[A\n",
            "Iteration:  26%|â–ˆâ–ˆâ–Œ       | 31/120 [00:09<00:28,  3.12it/s]\u001b[A\n",
            "Iteration:  27%|â–ˆâ–ˆâ–‹       | 32/120 [00:09<00:27,  3.20it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 33/120 [00:10<00:26,  3.27it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 34/120 [00:10<00:26,  3.30it/s]\u001b[A\n",
            "Iteration:  29%|â–ˆâ–ˆâ–‰       | 35/120 [00:10<00:25,  3.32it/s]\u001b[A\n",
            "Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:10<00:25,  3.34it/s]\u001b[A\n",
            "Iteration:  31%|â–ˆâ–ˆâ–ˆ       | 37/120 [00:11<00:24,  3.37it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–      | 38/120 [00:11<00:24,  3.39it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 39/120 [00:11<00:23,  3.42it/s]\u001b[A\n",
            "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:12<00:26,  3.00it/s]\u001b[A\n",
            "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–      | 41/120 [00:12<00:25,  3.12it/s]\u001b[A\n",
            "Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 42/120 [00:12<00:24,  3.20it/s]\u001b[A\n",
            "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 43/120 [00:13<00:23,  3.28it/s]\u001b[A\n",
            "Iteration:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:13<00:22,  3.32it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 45/120 [00:13<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [00:13<00:22,  3.36it/s]\u001b[A\n",
            "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/120 [00:14<00:21,  3.39it/s]\u001b[A\n",
            "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:14<00:21,  3.41it/s]\u001b[A\n",
            "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 49/120 [00:14<00:20,  3.41it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/120 [00:15<00:23,  3.01it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 51/120 [00:15<00:22,  3.11it/s]\u001b[A\n",
            "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:15<00:21,  3.22it/s]\u001b[A\n",
            "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/120 [00:16<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 54/120 [00:16<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [00:16<00:19,  3.36it/s]\u001b[A\n",
            "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/120 [00:17<00:18,  3.42it/s]\u001b[A\n",
            "Iteration:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/120 [00:17<00:17,  3.45it/s]\u001b[A\n",
            "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:18<00:19,  3.04it/s]\u001b[A\n",
            "Iteration:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 61/120 [00:18<00:18,  3.14it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [00:18<00:17,  3.24it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 63/120 [00:19<00:17,  3.32it/s]\u001b[A\n",
            "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:19<00:16,  3.34it/s]\u001b[A\n",
            "Iteration:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 65/120 [00:19<00:16,  3.38it/s]\u001b[A\n",
            "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 66/120 [00:20<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 67/120 [00:20<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:20<00:15,  3.40it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/120 [00:20<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 70/120 [00:21<00:16,  2.99it/s]\u001b[A\n",
            "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/120 [00:21<00:15,  3.12it/s]\u001b[A\n",
            "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:21<00:15,  3.20it/s]\u001b[A\n",
            "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 73/120 [00:22<00:14,  3.26it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74/120 [00:22<00:13,  3.31it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 75/120 [00:22<00:13,  3.36it/s]\u001b[A\n",
            "Iteration:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:23<00:12,  3.39it/s]\u001b[A\n",
            "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 77/120 [00:23<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 78/120 [00:23<00:12,  3.42it/s]\u001b[A\n",
            "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 79/120 [00:23<00:11,  3.43it/s]\u001b[A\n",
            "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:24<00:13,  3.01it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/120 [00:24<00:12,  3.12it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 82/120 [00:24<00:11,  3.22it/s]\u001b[A\n",
            "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/120 [00:25<00:11,  3.29it/s]\u001b[A\n",
            "Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:25<00:10,  3.31it/s]\u001b[A\n",
            "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 85/120 [00:25<00:10,  3.35it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 86/120 [00:26<00:10,  3.38it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 87/120 [00:26<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:26<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 89/120 [00:26<00:09,  3.40it/s]\u001b[A\n",
            "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [00:27<00:10,  3.00it/s]\u001b[A\n",
            "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 91/120 [00:27<00:09,  3.10it/s]\u001b[A\n",
            "Iteration:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:28<00:08,  3.20it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/120 [00:28<00:08,  3.26it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 94/120 [00:28<00:07,  3.32it/s]\u001b[A\n",
            "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 95/120 [00:28<00:07,  3.38it/s]\u001b[A\n",
            "Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:29<00:07,  3.41it/s]\u001b[A\n",
            "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 97/120 [00:29<00:06,  3.42it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [00:29<00:06,  3.44it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [00:30<00:06,  3.44it/s]\u001b[A\n",
            "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:30<00:06,  2.99it/s]\u001b[A\n",
            "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 101/120 [00:30<00:06,  3.10it/s]\u001b[A\n",
            "Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/120 [00:31<00:05,  3.17it/s]\u001b[A\n",
            "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 103/120 [00:31<00:05,  3.24it/s]\u001b[A\n",
            "Iteration:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:31<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 105/120 [00:31<00:04,  3.34it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [00:32<00:04,  3.38it/s]\u001b[A\n",
            "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 107/120 [00:32<00:03,  3.40it/s]\u001b[A\n",
            "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:32<00:03,  3.42it/s]\u001b[A\n",
            "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 109/120 [00:33<00:03,  3.43it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 110/120 [00:33<00:03,  3.02it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 111/120 [00:33<00:02,  3.13it/s]\u001b[A\n",
            "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:34<00:02,  3.23it/s]\u001b[A\n",
            "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/120 [00:34<00:02,  3.27it/s]\u001b[A\n",
            "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/120 [00:34<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 115/120 [00:34<00:01,  3.37it/s]\u001b[A\n",
            "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:35<00:01,  3.40it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 117/120 [00:35<00:00,  3.42it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 118/120 [00:35<00:00,  3.43it/s]\u001b[A\n",
            "Iteration:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 119/120 [00:36<00:00,  3.43it/s]\u001b[A\n",
            "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:36<00:00,  3.28it/s]\n",
            "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:26<00:36, 36.73s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:34,  3.43it/s]\u001b[A\n",
            "Iteration:   2%|â–         | 2/120 [00:00<00:34,  3.43it/s]\u001b[A\n",
            "Iteration:   2%|â–Ž         | 3/120 [00:00<00:34,  3.43it/s]\u001b[A\n",
            "Iteration:   3%|â–Ž         | 4/120 [00:01<00:33,  3.43it/s]\u001b[A\n",
            "Iteration:   4%|â–         | 5/120 [00:01<00:33,  3.43it/s]\u001b[A\n",
            "Iteration:   5%|â–Œ         | 6/120 [00:01<00:33,  3.45it/s]\u001b[A\n",
            "Iteration:   6%|â–Œ         | 7/120 [00:02<00:32,  3.47it/s]\u001b[A\n",
            "Iteration:   7%|â–‹         | 8/120 [00:02<00:32,  3.45it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 9/120 [00:02<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:   8%|â–Š         | 10/120 [00:03<00:36,  3.03it/s]\u001b[A\n",
            "Iteration:   9%|â–‰         | 11/120 [00:03<00:34,  3.14it/s]\u001b[A\n",
            "Iteration:  10%|â–ˆ         | 12/120 [00:03<00:33,  3.24it/s]\u001b[A\n",
            "Iteration:  11%|â–ˆ         | 13/120 [00:03<00:32,  3.29it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–        | 14/120 [00:04<00:31,  3.33it/s]\u001b[A\n",
            "Iteration:  12%|â–ˆâ–Ž        | 15/120 [00:04<00:31,  3.37it/s]\u001b[A\n",
            "Iteration:  13%|â–ˆâ–Ž        | 16/120 [00:04<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  14%|â–ˆâ–        | 17/120 [00:05<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  15%|â–ˆâ–Œ        | 18/120 [00:05<00:29,  3.40it/s]\u001b[A\n",
            "Iteration:  16%|â–ˆâ–Œ        | 19/120 [00:05<00:29,  3.42it/s]\u001b[A08/05/2020 15:56:33 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "08/05/2020 15:56:33 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "08/05/2020 15:56:33 - INFO - __main__ -     Num examples = 240\n",
            "08/05/2020 15:56:33 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2%|â–         | 2/120 [00:00<00:08, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|â–Ž         | 4/120 [00:00<00:08, 14.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|â–Œ         | 6/120 [00:00<00:08, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7%|â–‹         | 8/120 [00:00<00:08, 13.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|â–Š         | 10/120 [00:00<00:07, 14.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|â–ˆ         | 12/120 [00:00<00:07, 13.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|â–ˆâ–        | 14/120 [00:01<00:07, 13.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13%|â–ˆâ–Ž        | 16/120 [00:01<00:07, 14.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|â–ˆâ–Œ        | 18/120 [00:01<00:07, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17%|â–ˆâ–‹        | 20/120 [00:01<00:07, 14.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|â–ˆâ–Š        | 22/120 [00:01<00:06, 14.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|â–ˆâ–ˆ        | 24/120 [00:01<00:06, 14.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22%|â–ˆâ–ˆâ–       | 26/120 [00:01<00:06, 14.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:01<00:06, 14.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 30/120 [00:02<00:06, 14.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 32/120 [00:02<00:06, 14.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|â–ˆâ–ˆâ–Š       | 34/120 [00:02<00:06, 14.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:02<00:05, 14.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 38/120 [00:02<00:05, 14.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:02<00:05, 14.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 42/120 [00:02<00:05, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:03<00:05, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [00:03<00:05, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:03<00:05, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/120 [00:03<00:04, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:03<00:04, 14.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 54/120 [00:03<00:04, 14.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:03<00:04, 14.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/120 [00:04<00:04, 14.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:04<00:04, 14.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [00:04<00:04, 14.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:04<00:03, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 66/120 [00:04<00:03, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:04<00:03, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 70/120 [00:04<00:03, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:05<00:03, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74/120 [00:05<00:03, 14.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:05<00:03, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 78/120 [00:05<00:02, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:05<00:02, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 82/120 [00:05<00:02, 14.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:05<00:02, 14.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 86/120 [00:06<00:02, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:06<00:02, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [00:06<00:02, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:06<00:01, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 94/120 [00:06<00:01, 14.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:06<00:01, 14.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [00:06<00:01, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:07<00:01, 14.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/120 [00:07<00:01, 14.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:07<00:01, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [00:07<00:00, 14.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:07<00:00, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 110/120 [00:07<00:00, 14.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:07<00:00, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/120 [00:08<00:00, 14.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:08<00:00, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 118/120 [00:08<00:00, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:08<00:00, 14.09it/s]\n",
            "08/05/2020 15:56:41 - INFO - __main__ -   ***** Eval results  *****\n",
            "08/05/2020 15:56:41 - INFO - __main__ -     eval_loss = 1.9478546837965647\n",
            "08/05/2020 15:56:41 - INFO - __main__ -     perplexity = tensor(7.0136)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "08/05/2020 15:56:41 - INFO - transformers.configuration_utils -   Configuration saved in ./textgenmodels/checkpoint-50/config.json\n",
            "08/05/2020 15:56:47 - INFO - transformers.modeling_utils -   Model weights saved in ./textgenmodels/checkpoint-50/pytorch_model.bin\n",
            "08/05/2020 15:56:47 - INFO - __main__ -   Saving model checkpoint to ./textgenmodels/checkpoint-50\n",
            "\n",
            "Iteration:  17%|â–ˆâ–‹        | 20/120 [00:20<07:39,  4.59s/it]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 21/120 [00:20<05:28,  3.32s/it]\u001b[A\n",
            "Iteration:  18%|â–ˆâ–Š        | 22/120 [00:20<03:56,  2.41s/it]\u001b[A\n",
            "Iteration:  19%|â–ˆâ–‰        | 23/120 [00:21<02:51,  1.77s/it]\u001b[A\n",
            "Iteration:  20%|â–ˆâ–ˆ        | 24/120 [00:21<02:07,  1.33s/it]\u001b[A\n",
            "Iteration:  21%|â–ˆâ–ˆ        | 25/120 [00:21<01:36,  1.02s/it]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–       | 26/120 [00:22<01:15,  1.25it/s]\u001b[A\n",
            "Iteration:  22%|â–ˆâ–ˆâ–Ž       | 27/120 [00:22<01:00,  1.54it/s]\u001b[A\n",
            "Iteration:  23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:22<00:49,  1.85it/s]\u001b[A\n",
            "Iteration:  24%|â–ˆâ–ˆâ–       | 29/120 [00:22<00:42,  2.15it/s]\u001b[A\n",
            "Iteration:  25%|â–ˆâ–ˆâ–Œ       | 30/120 [00:23<00:40,  2.22it/s]\u001b[A\n",
            "Iteration:  26%|â–ˆâ–ˆâ–Œ       | 31/120 [00:23<00:36,  2.47it/s]\u001b[A\n",
            "Iteration:  27%|â–ˆâ–ˆâ–‹       | 32/120 [00:23<00:32,  2.71it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 33/120 [00:24<00:29,  2.90it/s]\u001b[A\n",
            "Iteration:  28%|â–ˆâ–ˆâ–Š       | 34/120 [00:24<00:28,  3.05it/s]\u001b[A\n",
            "Iteration:  29%|â–ˆâ–ˆâ–‰       | 35/120 [00:24<00:26,  3.15it/s]\u001b[A\n",
            "Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:25<00:25,  3.25it/s]\u001b[A\n",
            "Iteration:  31%|â–ˆâ–ˆâ–ˆ       | 37/120 [00:25<00:25,  3.28it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–      | 38/120 [00:25<00:24,  3.28it/s]\u001b[A\n",
            "Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 39/120 [00:25<00:24,  3.32it/s]\u001b[A\n",
            "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:26<00:26,  2.97it/s]\u001b[A\n",
            "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–      | 41/120 [00:26<00:25,  3.08it/s]\u001b[A\n",
            "Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 42/120 [00:27<00:24,  3.15it/s]\u001b[A\n",
            "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 43/120 [00:27<00:23,  3.23it/s]\u001b[A\n",
            "Iteration:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:27<00:23,  3.30it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 45/120 [00:27<00:22,  3.35it/s]\u001b[A\n",
            "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [00:28<00:21,  3.40it/s]\u001b[A\n",
            "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/120 [00:28<00:21,  3.38it/s]\u001b[A\n",
            "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:28<00:21,  3.41it/s]\u001b[A\n",
            "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 49/120 [00:29<00:20,  3.44it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/120 [00:29<00:23,  3.04it/s]\u001b[A\n",
            "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 51/120 [00:29<00:21,  3.14it/s]\u001b[A\n",
            "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:30<00:21,  3.20it/s]\u001b[A\n",
            "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/120 [00:30<00:20,  3.29it/s]\u001b[A\n",
            "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 54/120 [00:30<00:19,  3.34it/s]\u001b[A\n",
            "Iteration:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [00:30<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:31<00:18,  3.40it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [00:31<00:18,  3.43it/s]\u001b[A\n",
            "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/120 [00:31<00:18,  3.43it/s]\u001b[A\n",
            "Iteration:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/120 [00:32<00:17,  3.42it/s]\u001b[A\n",
            "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:32<00:19,  3.02it/s]\u001b[A\n",
            "Iteration:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 61/120 [00:32<00:18,  3.12it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [00:33<00:18,  3.21it/s]\u001b[A\n",
            "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 63/120 [00:33<00:17,  3.28it/s]\u001b[A\n",
            "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:33<00:16,  3.32it/s]\u001b[A\n",
            "Iteration:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 65/120 [00:33<00:16,  3.36it/s]\u001b[A\n",
            "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 66/120 [00:34<00:15,  3.39it/s]\u001b[A\n",
            "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 67/120 [00:34<00:15,  3.42it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:34<00:15,  3.42it/s]\u001b[A\n",
            "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/120 [00:35<00:14,  3.42it/s]\u001b[A\n",
            "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 70/120 [00:35<00:16,  3.01it/s]\u001b[A\n",
            "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/120 [00:35<00:15,  3.11it/s]\u001b[A\n",
            "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:36<00:15,  3.20it/s]\u001b[A\n",
            "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 73/120 [00:36<00:14,  3.28it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74/120 [00:36<00:13,  3.31it/s]\u001b[A\n",
            "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 75/120 [00:36<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:37<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 77/120 [00:37<00:12,  3.40it/s]\u001b[A\n",
            "Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 78/120 [00:37<00:12,  3.40it/s]\u001b[A\n",
            "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 79/120 [00:38<00:11,  3.42it/s]\u001b[A\n",
            "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:38<00:13,  3.02it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/120 [00:38<00:12,  3.09it/s]\u001b[A\n",
            "Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 82/120 [00:39<00:11,  3.20it/s]\u001b[A\n",
            "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/120 [00:39<00:11,  3.25it/s]\u001b[A\n",
            "Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:39<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 85/120 [00:40<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 86/120 [00:40<00:10,  3.34it/s]\u001b[A\n",
            "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 87/120 [00:40<00:09,  3.35it/s]\u001b[A\n",
            "Iteration:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:40<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 89/120 [00:41<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [00:41<00:09,  3.01it/s]\u001b[A\n",
            "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 91/120 [00:41<00:09,  3.11it/s]\u001b[A\n",
            "Iteration:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:42<00:08,  3.21it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/120 [00:42<00:08,  3.29it/s]\u001b[A\n",
            "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 94/120 [00:42<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 95/120 [00:43<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:43<00:07,  3.38it/s]\u001b[A\n",
            "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 97/120 [00:43<00:06,  3.39it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [00:44<00:07,  3.09it/s]\u001b[A\n",
            "Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [00:44<00:06,  3.17it/s]\u001b[A\n",
            "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:44<00:06,  2.87it/s]\u001b[A\n",
            "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 101/120 [00:45<00:06,  2.98it/s]\u001b[A\n",
            "Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/120 [00:45<00:05,  3.10it/s]\u001b[A\n",
            "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 103/120 [00:45<00:05,  3.17it/s]\u001b[A\n",
            "Iteration:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:45<00:04,  3.24it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 105/120 [00:46<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [00:46<00:04,  3.32it/s]\u001b[A\n",
            "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 107/120 [00:46<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:47<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 109/120 [00:47<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 110/120 [00:47<00:03,  3.00it/s]\u001b[A\n",
            "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 111/120 [00:48<00:02,  3.07it/s]\u001b[A\n",
            "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:48<00:02,  3.18it/s]\u001b[A\n",
            "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/120 [00:48<00:02,  3.25it/s]\u001b[A\n",
            "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/120 [00:49<00:01,  3.30it/s]\u001b[A\n",
            "Iteration:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 115/120 [00:49<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:49<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 117/120 [00:49<00:00,  3.37it/s]\u001b[A\n",
            "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 118/120 [00:50<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 119/120 [00:50<00:00,  3.41it/s]\u001b[A\n",
            "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:50<00:00,  2.36it/s]\n",
            "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.51s/it]\n",
            "08/05/2020 15:57:18 - INFO - __main__ -    global_step = 60, average loss = 3.3659872110933065\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58WmWwxiW5NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "3cba9e72-fa54-4f7a-95ff-48651aaea063"
      },
      "source": [
        "# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(parser, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = model_class.from_pretrained(args.output_dir)\n",
        "    tokenizer = YTEncoder.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:57:18 - INFO - __main__ -   Saving model checkpoint to ./textgenmodels\n",
            "08/05/2020 15:57:18 - INFO - transformers.configuration_utils -   Configuration saved in ./textgenmodels/config.json\n",
            "08/05/2020 15:57:26 - INFO - transformers.modeling_utils -   Model weights saved in ./textgenmodels/pytorch_model.bin\n",
            "08/05/2020 15:57:26 - INFO - transformers.configuration_utils -   loading configuration file ./textgenmodels/config.json\n",
            "08/05/2020 15:57:26 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"output_past\": true,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/05/2020 15:57:26 - INFO - transformers.modeling_utils -   loading weights file ./textgenmodels/pytorch_model.bin\n",
            "08/05/2020 15:57:40 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "08/05/2020 15:57:40 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./textgenmodels.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiU3UtCZjloQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a2d00e22-8ae8-4ca1-f540-3467862f33cf"
      },
      "source": [
        "CONTEXT_TEXT = ''\n",
        "\n",
        "START_TEXT = 'Ð¢Ñ‹ Ñ‡Ñ‘ '\n",
        "CONTEXT_TEXT = CONTEXT_TEXT + START_TEXT\n",
        "\n",
        "context_tokens = tokenizer.encode(CONTEXT_TEXT)\n",
        "sampled = sample_sequence(model, \n",
        "                          100, \n",
        "                          context_tokens, \n",
        "                          temperature = 1.0,\n",
        "                          top_p=0.80\n",
        "                          )\n",
        "\n",
        "out = sampled[:, len(context_tokens):].tolist()\n",
        "text = ''.join([tokenizer.decode(o) for o in out])\n",
        "\n",
        "# text = text[: text.find('<| endoftext|>')].split('\\n')\n",
        "text = text.split('\\n')\n",
        "\n",
        "print('-' * 20)\n",
        "\n",
        "for i, t in enumerate(text):\n",
        "    if i == 0:\n",
        "        print(START_TEXT + t)\n",
        "    else:\n",
        "        print(t)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 19.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Ð¢Ñ‹ Ñ‡Ñ‘ Ñ â€” ÐºÐ¾ÑÐ¼Ð¾Ð¿Ð¾Ð»Ð¸Ñ‚\n",
            "ÐÐ° Ð½ÐµÐ±Ðµ Ð´Ñ‹Ð¼, Ð¿Ð¾Ð´ Ð½Ð¸Ð¼ Ð±ÐµÑ‚Ð¾Ð½\n",
            "Ð¯ Ð¿Ð¾Ð´Ð½Ð¸Ð¼Ð°ÑŽÑÑŒ, ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ñ Ð³Ð¾Ñ€Ñ‹\n",
            "Ð¯ Ð»ÐµÐ·Ñƒ Ð¸ Ð»ÐµÐ·Ñƒ Ð½Ð°Ð²ÐµÑ€Ñ…\n",
            "Ð¢Ñ‹ Ð³Ð¾Ð²Ð¾Ñ€Ð¸ÑˆÑŒ â€” Â«ÐžÐ½ ÑÐ³Ð¾Ñ†ÐµÐ½Ñ‚Ñ€Ð¸ÐºÂ»,\n",
            "ÐÐ¾ Ð¼Ð¾Ð¹ ÑÐ³Ð¾Ð¸Ð·Ð¼ â€” ÑÑ‚Ð¾ Ñ‚Ñ‹\n",
            "Ð¢Ñ‹ Ñ…Ð¾Ñ‡ÐµÑˆÑŒ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð¼ÐµÐ½Ñ Ð¼Ñ‹ÑˆÑŒÑŽ? Ð¯ â€” Ð¿Ð°Ð½Ñ‚ÐµÑ€Ð°!\n",
            "<| endoftext|>\n",
            "Ð’ÑÑ‘ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¸Ð´ÐµÐ°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾, ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¿Ñ€Ð¸ÑÑ‚Ñ€Ð°ÑÑ‚Ð½Ð¾\n",
            "Ð”Ð¾ ÑÐ¸Ñ… Ð¿Ð¾Ñ€ Ð´ÐµÐºÐ°Ð´ÐµÐ½Ñ‚ÑÑ‚Ð²Ð¾ â€” ÑÑ‚Ð¾ Ð½Ð¾Ð½ÑÐµÐ½Ñ\n",
            "Ð Ð¼Ñ‹ â€” Ð´ÐµÑ‚Ð¸ Ñ€ÐµÐ¿Ð°Ñ‚Ñ€Ð¸Ð°Ð½Ñ‚Ð°\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iekxNtc-qYkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! rm -rf ./textgenmodels/checkpoint-50\n",
        "# ! zip -r res_oxxxymiron.zip textgenmodels"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ29bAwsdekz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtXg1Bgtdk4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! cp ./res_oxxxymiron.zip './gdrive/My Drive/gpt2/res_oxxxymiron.zip'"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}