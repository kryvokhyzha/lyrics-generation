{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Transformers Generation Training RU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Blks7HWN9Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MQRI-JmN9Ls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d0294c9f-b580-4b45-86a3-dc5a176095b8"
      },
      "source": [
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7DrdgrQB8ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_qikR-QOHcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d11c05c8-c083-4851-8a27-1ae2346f52a3"
      },
      "source": [
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        " \n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 159.3 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhsqSD6GRE2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b6b94544-4f7e-4685-d845-e9fad7d42621"
      },
      "source": [
        "! pip install awscli\n",
        "! aws s3 sync --no-sign-request s3://models.dobro.ai/gpt2/ru/unfreeze_all gpt2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: awscli in /usr/local/lib/python3.6/dist-packages (1.18.112)\n",
            "Requirement already satisfied: rsa<=4.5.0,>=3.1.2; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (4.5)\n",
            "Requirement already satisfied: botocore==1.17.35 in /usr/local/lib/python3.6/dist-packages (from awscli) (1.17.35)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.15.2)\n",
            "Requirement already satisfied: PyYAML<5.4,>=3.10; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (3.13)\n",
            "Requirement already satisfied: colorama<0.4.4,>=0.2.5; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (0.4.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.3.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=4.5.0,>=3.1.2; python_version != \"3.4\"->awscli) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.35->awscli) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.35->awscli) (2.8.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.35->awscli) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.17.35->awscli) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lICQFj-Fn4uZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b9a62036-021b-4ad3-c0f3-94e377a0999c"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-yi_yBX-O2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b46180b-9f28-4206-b2d1-ea783c74a010"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFlCwY4BZgcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7675bbaf-ddad-4a78-ef34-b04a3dcbfe26"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-rdldjzt7\n",
            "Created temporary directory: /tmp/pip-req-tracker-fhev891_\n",
            "Created requirements tracker '/tmp/pip-req-tracker-fhev891_'\n",
            "Created temporary directory: /tmp/pip-install-9io01qyy\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-6q45_fds\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-fhev891_'\n",
            "    Running setup.py (path:/tmp/pip-req-build-6q45_fds/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-6q45_fds/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-6q45_fds/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-6q45_fds has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-fhev891_'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-tgxecy8f\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-tgxecy8f\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-6q45_fds/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-6q45_fds/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-tgxecy8f --python-tag cp36\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-6q45_fds/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-tgxecy8f/apex-0.1-cp36-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp36-none-any.whl size=192740 sha256=c52142cec217f47cf84f02b85d8f0b3a1223715554b5f33d78bc7da3c559020a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rdldjzt7/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-6q45_fds\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex-0.1.dist-info\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex-0.1.dist-info/\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex/\n",
            "      Successfully uninstalled apex-0.1\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-fhev891_'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-QowDzeSit5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fdd0591-8eca-429b-cdca-ecd303b3134f"
      },
      "source": [
        "! pip install youtokentome"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtokentome in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KoFOAl_n--x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "# from tqdm import tqdm as tqdm_base\n",
        "# def tqdm(*args, **kwargs):\n",
        "#     if hasattr(tqdm_base, '_instances'):\n",
        "#         for instance in list(tqdm_base._instances):\n",
        "#             tqdm_base._decr_instances(instance)\n",
        "#     return tqdm_base(*args, **kwargs)\n",
        "\n",
        "from transformers import (WEIGHTS_NAME, AdamW, \n",
        "                          # WarmupLinearSchedule,\n",
        "                                  BertConfig, BertForMaskedLM, BertTokenizer,\n",
        "                                  GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
        "                                  OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
        "                                  RobertaConfig, RobertaForMaskedLM, RobertaTokenizer,\n",
        "                                  DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whcB1oX_o5jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger.setLevel('INFO')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7LgWLPwoDgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
        "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
        "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-EysG53iHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dict2obj(d):\n",
        "  if isinstance(d, list):\n",
        "    d = [dict2obj(x) for x in d]\n",
        "  if not isinstance(d, dict):\n",
        "    return d\n",
        "  class C(object):\n",
        "    pass\n",
        "  o = C()\n",
        "  for k in d:\n",
        "    o.__dict__[k] = dict2obj(d[k])\n",
        "  return o\n",
        "\n",
        "BLOCK_SIZE = 256\n",
        "\n",
        "parser = {}\n",
        "\n",
        "parser['train_data_file'] = './oxxxymiron_lyrics_end_text.txt' #'Lyrics_Земфира (Zemfira).txt'\n",
        "parser['input_dir'] = './gpt2/m_checkpoint-3364613'\n",
        "parser['output_dir'] = './textgenmodels'\n",
        "\n",
        "parser['eval_data_file'] = './oxxxymiron_lyrics_end_text.txt' #'Lyrics_Земфира (Zemfira).txt'\n",
        "parser['model_type'] = 'gpt2' # bert\n",
        "parser['model_name_or_path'] = 'gpt2-medium' # 'bert-base-cased'\n",
        "parser['mlm'] = False \n",
        "parser['mlm_probability'] = False\n",
        "\n",
        "parser['config_name'] = \"\"\n",
        "parser['tokenizer_name'] = \"\"\n",
        "parser['cache_dir'] = \"\"\n",
        "parser['block_size'] = BLOCK_SIZE\n",
        "parser['do_train'] = True\n",
        "parser['do_eval'] = True\n",
        "parser['evaluate_during_training'] = True\n",
        "parser['do_lower_case'] = True\n",
        "\n",
        "parser['per_gpu_train_batch_size'] = 2\n",
        "parser['per_gpu_eval_batch_size'] = 2\n",
        "parser['gradient_accumulation_steps'] = 10\n",
        "parser['learning_rate'] = 0.001 # 5e-5\n",
        "parser['weight_decay'] = 0.0\n",
        "parser['adam_epsilon'] = 1e-8\n",
        "parser['max_grad_norm'] = 1.0\n",
        "parser['num_train_epochs'] = 5.0\n",
        "parser['max_steps'] = -1\n",
        "parser['warmup_steps'] = 100\n",
        "\n",
        "parser['logging_steps'] = 50\n",
        "parser['save_steps'] = 50\n",
        "parser['save_total_limit'] = None\n",
        "parser['eval_all_checkpoints'] = True\n",
        "parser['no_cuda'] = False\n",
        "parser['overwrite_output_dir'] = True\n",
        "parser['overwrite_cache'] = True\n",
        "parser['seed'] = 42\n",
        "\n",
        "parser['fp16'] = True\n",
        "parser['fp16_opt_level'] = 'O1'\n",
        "parser['local_rank'] = -1\n",
        "parser['server_ip'] = \"\"\n",
        "parser['server_port'] = \"\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c1oL47oVsY",
        "colab_type": "text"
      },
      "source": [
        "# Data loading\n",
        "https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdxejtgroZnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path='train', block_size=BLOCK_SIZE):\n",
        "        assert os.path.isfile(file_path)\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(directory, 'cached_lm_' + str(block_size) + '_' + filename)\n",
        "\n",
        "        if os.path.exists(cached_features_file):\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'rb') as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "            tokenized_text = tokenizer.encode(text)\n",
        "\n",
        "            # TODO FIX WARNINGS WHERE SPECIAL TOKENS AND GPT2 OUTPUT TOO MUCH\n",
        "            for i in range(0, len(tokenized_text)-block_size+1, block_size): # Truncate in block of block_size\n",
        "                if parser['model_type'] == 'gpt2':\n",
        "                    self.examples.append(tokenized_text[i:i+block_size])\n",
        "                else:\n",
        "                    self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
        "                \n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'wb') as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7c504apCGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    dataset = TextDataset(tokenizer, file_path=args.eval_data_file if evaluate else args.train_data_file, block_size=args.block_size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix, use_mtime=False):\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, '{}-*'.format(checkpoint_prefix)))\n",
        "    if len(glob_checkpoints) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    ordering_and_checkpoint_path = []\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match('.*{}-([0-9]+)'.format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def mask_tokens(inputs, tokenizer, args):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
        "    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    # scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps = -1)\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
        "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = 'checkpoint'\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, '{}-{}'.format(checkpoint_prefix, global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    torch.save(parser, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\n",
        "        \"perplexity\": perplexity,\n",
        "        'eval_loss': eval_loss\n",
        "    }\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1AhHCpyp1MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# args = parser.parse_args()\n",
        "args = dict2obj(parser)\n",
        "\n",
        "if args.model_type in [\"bert\", \"roberta\", \"distilbert\"] and not args.mlm:\n",
        "  raise ValueError(\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
        "                    \"flag (masked language modeling).\")\n",
        "if args.eval_data_file is None and args.do_eval:\n",
        "  raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "                    \"or remove the --do_eval argument.\")\n",
        "\n",
        "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "  raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAOzVvOVqTSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup distant debugging if needed\n",
        "if args.server_ip and args.server_port:\n",
        "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "    import ptvsd\n",
        "    print(\"Waiting for debugger attach\")\n",
        "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "    ptvsd.wait_for_attach()\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "args.device = device"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qx4FyooWSu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3ac3b51-191f-481d-deec-ec3e9e93f07b"
      },
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "# Set seed\n",
        "set_seed(args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:53:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tgHKPySRBZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "import os\n",
        "import youtokentome as yttm\n",
        "import hashlib\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "import shutil\n",
        "import regex as re\n",
        "from os.path import samefile"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRwvZZnYRB4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NEW_LINE = '<|n|>'\n",
        "\n",
        "class YTEncoder(PreTrainedTokenizer):\n",
        "    def_name = 'encoder.model'\n",
        "    def __init__(self, filename, *inputs, **kwargs):\n",
        "        super().__init__(*inputs, **kwargs)\n",
        "        #self.max_len_single_sentence = BLOCK_SIZE # no default special tokens - you can update this value if you add special tokens\n",
        "        #self.max_len_sentences_pair = BLOCK_SIZE # no default special tokens - you can update this value if you add special tokens\n",
        "\n",
        "        if os.path.isdir(filename): filename = os.path.join(filename, self.def_name)\n",
        "\n",
        "        self.bpe = yttm.BPE(filename)\n",
        "        self.hash = hashlib.sha512(open(filename, 'rb').read()).hexdigest()[:10]\n",
        "        self.filename = filename\n",
        "\n",
        "    def encode(self, text):\n",
        "        if text and text[0] != ' ': text = ' ' + text\n",
        "        text = re.sub(r'(?=[^ ])([\\W])([\\w])',r'\\g<1> \\g<2>',text)\n",
        "        text = text.replace('\\n', f' {NEW_LINE} ')\n",
        "\n",
        "        return self.bpe.encode([text], output_type=yttm.OutputType.ID)[0]\n",
        "\n",
        "\n",
        "    def decode(self, tokens): # I hate regexps\n",
        "        if not isinstance(tokens,list):\n",
        "            tokens = tokens.tolist()\n",
        "        result = self.bpe.decode(tokens)[0]\n",
        "        result = re.sub(r'( )?(<\\|n\\|>)( )?', r'\\n', result)\n",
        "        result = re.sub(r'([\\n(]) (\\w)',r'\\g<1>\\g<2>', result)\n",
        "        result = re.sub(r'(\\W)([«\"''\\n(]|^) (\\w)',r'\\g<1>\\g<2>\\g<3>', result)\n",
        "        result = re.sub(r'(\\w)- (\\w)',r'\\g<1>-\\g<2>', result)\n",
        "        return result\n",
        "\n",
        "    def tokenize(self, text, **kwargs):\n",
        "        return self.encode(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *inputs, **kwargs):\n",
        "        return cls(*inputs, **kwargs)\n",
        "\n",
        "    def add_special_tokens_single_sentence(self, token_ids):\n",
        "        return token_ids\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        src = self.filename\n",
        "        dst = os.path.join(save_directory, self.def_name)\n",
        "        if src != dst:\n",
        "            shutil.copyfile(src, dst)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcdDevtBWVus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "a553b62a-3bbd-4457-c19d-c6bd5d5152cb"
      },
      "source": [
        "# Load pretrained model and tokenizer\n",
        "if args.local_rank not in [-1, 0]:\n",
        "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "\n",
        "model = model_class.from_pretrained(args.input_dir)\n",
        "tokenizer = YTEncoder.from_pretrained(args.input_dir)\n",
        "model.to(args.device)\n",
        "\n",
        "if args.block_size <= 0:\n",
        "    args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "\n",
        "if args.local_rank == 0:\n",
        "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "logger.info(\"Training/evaluation parameters %s\", args)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:53:14 - INFO - transformers.configuration_utils -   loading configuration file ./gpt2/m_checkpoint-3364613/config.json\n",
            "08/05/2020 15:53:14 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"output_past\": true,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/05/2020 15:53:14 - INFO - transformers.modeling_utils -   loading weights file ./gpt2/m_checkpoint-3364613/pytorch_model.bin\n",
            "08/05/2020 15:53:28 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "08/05/2020 15:53:28 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at ./gpt2/m_checkpoint-3364613 and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "08/05/2020 15:53:39 - INFO - __main__ -   Training/evaluation parameters <__main__.dict2obj.<locals>.C object at 0x7fc43a333828>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv39Fsh-ZshR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size x vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None, xlm_lang=None, device='cuda'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            if is_xlnet: \n",
        "                # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
        "                # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
        "                input_ids = torch.cat((generated, torch.zeros((1, 1), dtype=torch.long, device=device)), dim=1)\n",
        "                perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "                perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
        "                target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "                target_mapping[0, 0, -1] = 1.0  # predict last token\n",
        "                inputs = {'input_ids': input_ids, 'perm_mask': perm_mask, 'target_mapping': target_mapping}\n",
        "\n",
        "            if is_xlm_mlm and xlm_mask_token:\n",
        "                # XLM MLM models are direct models (predict same token, not next token)\n",
        "                # => need one additional dummy token in the input (will be masked and guessed)\n",
        "                input_ids = torch.cat((generated, torch.full((1, 1), xlm_mask_token, dtype=torch.long, device=device)), dim=1)\n",
        "                inputs = {'input_ids': input_ids}\n",
        "\n",
        "            if xlm_lang is not None:\n",
        "                inputs[\"langs\"] = torch.tensor([xlm_lang] * inputs[\"input_ids\"].shape[1], device=device).view(1, -1)\n",
        "\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1rM-f2YZVtb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "7fd0c557-142d-4c90-84d8-35c0a8cd5b37"
      },
      "source": [
        "# CONTEXT_TEXT = 'Всё ещё бабла нет, всё ещё с долгами канитель \\n \\\n",
        "# Все ещё в подвале, всё ещё Parliament на бите \\n \\\n",
        "# И я вернусь на трек, твой хуй как Тулуз-Лотрек \\n \\\n",
        "# Если русский рэп в гробу сто лет, то я ебу скелет \\n \\\n",
        "# И я построил альбом на костях \\n \\\n",
        "# Этому не видно конца, как будто он голый толстяк \\n \\\n",
        "# Каждый просит фит, каждый пишет: «Денег дам» \\n \\\n",
        "# Вас миллион, но мой кумир – Гриша Перельман \\n'\n",
        "\n",
        "CONTEXT_TEXT = ''\n",
        "START_TEXT = 'Я здесь'\n",
        "CONTEXT_TEXT += START_TEXT\n",
        "\n",
        "context_tokens = tokenizer.encode(CONTEXT_TEXT)\n",
        "sampled = sample_sequence(model, \n",
        "                          100, \n",
        "                          context_tokens, \n",
        "                          temperature = 1.0,\n",
        "                          top_p=0.99\n",
        "                          )\n",
        "\n",
        "out = sampled[:, len(context_tokens):].tolist()\n",
        "text = ''.join([tokenizer.decode(o) for o in out])\n",
        "text = text[: text.find('<|endoftext|>')].split('\\n')\n",
        "\n",
        "print('-' * 20)\n",
        "\n",
        "for i, t in enumerate(text):\n",
        "    if i == 0:\n",
        "        print(START_TEXT + t)\n",
        "    else:\n",
        "        print(t)\n",
        "\n",
        "evaluate(args, model, tokenizer)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:03<00:00, 33.19it/s]\n",
            "08/05/2020 15:53:42 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "08/05/2020 15:53:42 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "08/05/2020 15:53:42 - INFO - __main__ -     Num examples = 240\n",
            "08/05/2020 15:53:42 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating:   1%|          | 1/120 [00:00<00:15,  7.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Я здесьпотому, что я его боюсь. Не потому что боюсь Бонуса, Джека. Сутки назад я согласилась надеть эти сапоги потому, что после выборов мы по очереди выходим в фабрику. Я уже одеваю их на слишком большую протяженность работ. Конечно, мистер Бонус — слишком крутой, чтобы его меня найти. Я бы не надела эти сапоги на улицу, чтобы никого там не увидеть, я не это имела в виду, но однажды я посмотрела в глазок — таких сапог здесь только два. («... и больше ничего?»\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 120/120 [00:17<00:00,  6.88it/s]\n",
            "08/05/2020 15:54:00 - INFO - __main__ -   ***** Eval results  *****\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     eval_loss = 4.464891692002614\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     perplexity = tensor(86.9116)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 4.464891692002614, 'perplexity': tensor(86.9116)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anYKS2-8WZnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "511eba1d-eee1-4ba7-e108-f2a39bb30e2c"
      },
      "source": [
        "# Training\n",
        "if args.do_train:\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:54:00 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "08/05/2020 15:54:00 - INFO - __main__ -   ***** Running training *****\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Num examples = 240\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Num Epochs = 5\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 20\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Gradient Accumulation steps = 10\n",
            "08/05/2020 15:54:00 - INFO - __main__ -     Total optimization steps = 60\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/120 [00:00<00:31,  3.77it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:32,  3.64it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:32,  3.56it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:33,  3.50it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:32,  3.49it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:32,  3.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.44it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:36,  2.97it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:35,  3.05it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:34,  3.14it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:33,  3.21it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:32,  3.28it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:31,  3.35it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.39it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:33,  3.05it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:32,  3.15it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:35,  2.84it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:33,  2.95it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:31,  3.07it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:07<00:30,  3.16it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:29,  3.22it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:29,  3.25it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:08<00:28,  3.30it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.33it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:27,  3.36it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:27,  3.34it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:30,  2.97it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:28,  3.09it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:27,  3.16it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:10<00:26,  3.23it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:26,  3.25it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.28it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:11<00:25,  3.32it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.32it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.34it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:12<00:24,  3.36it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:27,  2.95it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:25,  3.05it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:13<00:24,  3.12it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:13<00:24,  3.19it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:23,  3.23it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:23,  3.25it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:14<00:22,  3.29it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.33it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:21,  3.34it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:15<00:21,  3.36it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:23,  2.98it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:22,  3.06it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:16<00:21,  3.10it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:16<00:21,  3.18it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:20,  3.23it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:17<00:19,  3.26it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:17<00:19,  3.30it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:19,  3.28it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.32it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:18<00:18,  3.34it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:20,  2.95it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:19,  3.05it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:19<00:18,  3.14it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:19<00:17,  3.21it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:17,  3.26it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:20<00:16,  3.28it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:20<00:16,  3.31it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.32it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:21<00:15,  3.35it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:21<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:16,  2.97it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:22<00:15,  3.08it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:22<00:15,  3.16it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:22<00:14,  3.21it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:14,  3.25it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:23<00:13,  3.30it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:23<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:24<00:12,  3.39it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:24<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:13,  3.00it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:25<00:12,  3.11it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:25<00:11,  3.19it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.24it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:26<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:26<00:10,  3.32it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:26<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.36it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:27<00:09,  3.36it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:27<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:10,  2.98it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:28<00:09,  3.09it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:28<00:08,  3.16it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:29<00:07,  3.29it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:29<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:29<00:07,  3.38it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.37it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:30<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:30<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:31<00:06,  3.00it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:31<00:06,  3.10it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:31<00:05,  3.18it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.25it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:32<00:04,  3.30it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:32<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:32<00:04,  3.39it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:33<00:03,  3.41it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:33<00:03,  3.41it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:33<00:03,  3.43it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:34<00:03,  3.02it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:34<00:02,  3.11it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:34<00:02,  3.18it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.26it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:35<00:01,  3.31it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:35<00:01,  3.36it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:35<00:01,  3.38it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:36<00:00,  3.39it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:36<00:00,  3.41it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:36<00:00,  3.41it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:37<00:00,  3.23it/s]\n",
            "Epoch:  20%|██        | 1/5 [00:37<02:28, 37.12s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.32it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:35,  3.35it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.39it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:34,  3.40it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:36,  3.04it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:35,  3.10it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.20it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.27it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:30,  3.38it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:29,  3.41it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.44it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:33,  3.02it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:31,  3.13it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:30,  3.23it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.30it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.36it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:27,  3.40it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:26,  3.43it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:26,  3.45it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:29,  3.04it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:28,  3.13it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:27,  3.22it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:09<00:26,  3.31it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:25,  3.37it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.38it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:24,  3.40it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.43it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:23,  3.44it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:23,  3.46it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:25,  3.09it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:24,  3.19it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:23,  3.26it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:12<00:23,  3.32it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.36it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.41it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:21,  3.43it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.43it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:20,  3.44it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:20,  3.47it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:23,  3.03it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:21,  3.14it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:21,  3.24it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:15<00:20,  3.31it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.34it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.38it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:16<00:18,  3.40it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.43it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.42it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:17,  3.43it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:19,  3.06it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:18,  3.17it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:17,  3.25it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:19<00:17,  3.32it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:16,  3.37it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.39it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:19<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.44it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.45it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:14,  3.45it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:16,  3.02it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.13it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:14,  3.22it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:22<00:14,  3.29it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:13,  3.34it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:22<00:12,  3.42it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.45it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.45it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:11,  3.46it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:13,  3.06it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.13it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.22it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.31it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.40it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:25<00:09,  3.44it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.45it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.47it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:26<00:08,  3.45it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:09,  3.04it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:09,  3.16it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:27<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.30it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:07,  3.36it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.39it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:28<00:07,  3.42it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.43it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.45it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:29<00:06,  3.47it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  3.08it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:05,  3.18it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:30<00:05,  3.26it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.32it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.37it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:31<00:04,  3.36it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:31<00:04,  3.40it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.43it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.45it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:32<00:03,  3.46it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  3.05it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.14it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:33<00:02,  3.22it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.30it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:34<00:01,  3.37it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:34<00:01,  3.40it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.43it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.45it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:35<00:00,  3.47it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.31it/s]\n",
            "Epoch:  40%|████      | 2/5 [01:13<01:50, 36.86s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:34,  3.44it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:34,  3.44it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:33,  3.46it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:33,  3.47it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:32,  3.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:31,  3.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:35,  3.06it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:34,  3.17it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.24it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:30,  3.44it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:29,  3.46it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:29,  3.46it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.42it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:33,  3.02it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:31,  3.14it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:30,  3.22it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.28it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.34it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:28,  3.38it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.39it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:26,  3.43it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:26,  3.43it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:29,  3.06it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:28,  3.16it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:27,  3.25it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:09<00:26,  3.30it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:25,  3.34it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.37it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:24,  3.39it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.40it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.41it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:23,  3.43it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:26,  3.03it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:25,  3.14it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:24,  3.23it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:12<00:23,  3.29it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.32it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.38it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:21,  3.40it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.42it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:20,  3.43it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:20,  3.44it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:23,  3.03it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:22,  3.12it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:21,  3.20it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:15<00:20,  3.25it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.31it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:16<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.35it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:17,  3.40it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:22,  2.71it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:20,  2.89it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:19,  3.03it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:19<00:18,  3.12it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:17,  3.22it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.29it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:20<00:16,  3.31it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.33it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:15,  3.39it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:16,  3.01it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.13it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:14,  3.20it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:22<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:13,  3.33it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.37it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:23<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.39it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:13,  3.03it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.12it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.20it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.25it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.29it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.33it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:26<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.38it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:27<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:10,  2.97it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:09,  3.07it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:28<00:08,  3.17it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:07,  3.28it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.32it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:29<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.35it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.36it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:30<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  2.97it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:06,  3.06it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:31<00:05,  3.17it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.22it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:32<00:04,  3.31it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:32<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:33<00:03,  3.40it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  2.99it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.08it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:34<00:02,  3.18it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.24it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.28it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:35<00:01,  3.31it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:35<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.39it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:36<00:00,  3.41it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.27it/s]\n",
            "Epoch:  60%|██████    | 3/5 [01:50<01:13, 36.81s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.33it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:35,  3.37it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.39it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:34,  3.39it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:36,  3.03it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:34,  3.13it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.22it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.28it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.33it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.41it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:29,  3.43it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.44it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:33,  3.00it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:31,  3.11it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:30,  3.20it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.27it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.31it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:28,  3.33it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.37it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.40it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:27,  3.41it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:26,  3.42it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:29,  3.02it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:28,  3.12it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:27,  3.20it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:10<00:26,  3.27it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:26,  3.30it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.32it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:25,  3.34it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.37it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.39it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:23,  3.42it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:26,  3.00it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:25,  3.12it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:24,  3.20it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:13<00:23,  3.28it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.32it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:22,  3.36it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.39it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:21,  3.41it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:20,  3.41it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:23,  3.01it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:22,  3.11it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:21,  3.22it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:16<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.36it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.42it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:17,  3.45it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:19,  3.04it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:18,  3.14it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:17,  3.24it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:19<00:17,  3.32it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:16,  3.34it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.38it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:20<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.40it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:16,  2.99it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.12it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:15,  3.20it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:22<00:14,  3.26it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:13,  3.31it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.36it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:23<00:12,  3.39it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.41it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.42it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:11,  3.43it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:13,  3.01it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.12it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.22it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.29it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.31it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.35it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:26<00:10,  3.38it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:26<00:09,  3.40it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:10,  3.00it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:09,  3.10it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:28<00:08,  3.20it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.26it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:07,  3.32it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.38it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:29<00:07,  3.41it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.42it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.44it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:30<00:06,  3.44it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  2.99it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:06,  3.10it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:31<00:05,  3.17it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.24it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:31<00:04,  3.34it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:32<00:04,  3.38it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.40it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.42it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:33<00:03,  3.43it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  3.02it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.13it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:34<00:02,  3.23it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.27it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:34<00:01,  3.37it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:35<00:01,  3.40it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.42it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.43it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:36<00:00,  3.43it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.28it/s]\n",
            "Epoch:  80%|████████  | 4/5 [02:26<00:36, 36.73s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:34,  3.43it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:34,  3.43it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.43it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:33,  3.43it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.43it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:32,  3.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:36,  3.03it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:34,  3.14it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.24it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.29it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.33it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.37it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:29,  3.40it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.42it/s]\u001b[A08/05/2020 15:56:33 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "08/05/2020 15:56:33 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "08/05/2020 15:56:33 - INFO - __main__ -     Num examples = 240\n",
            "08/05/2020 15:56:33 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2%|▏         | 2/120 [00:00<00:08, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 4/120 [00:00<00:08, 14.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|▌         | 6/120 [00:00<00:08, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7%|▋         | 8/120 [00:00<00:08, 13.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|▊         | 10/120 [00:00<00:07, 14.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|█         | 12/120 [00:00<00:07, 13.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 14/120 [00:01<00:07, 13.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13%|█▎        | 16/120 [00:01<00:07, 14.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▌        | 18/120 [00:01<00:07, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17%|█▋        | 20/120 [00:01<00:07, 14.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 22/120 [00:01<00:06, 14.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|██        | 24/120 [00:01<00:06, 14.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22%|██▏       | 26/120 [00:01<00:06, 14.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23%|██▎       | 28/120 [00:01<00:06, 14.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25%|██▌       | 30/120 [00:02<00:06, 14.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27%|██▋       | 32/120 [00:02<00:06, 14.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 34/120 [00:02<00:06, 14.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|███       | 36/120 [00:02<00:05, 14.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32%|███▏      | 38/120 [00:02<00:05, 14.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|███▎      | 40/120 [00:02<00:05, 14.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|███▌      | 42/120 [00:02<00:05, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 44/120 [00:03<00:05, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38%|███▊      | 46/120 [00:03<00:05, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|████      | 48/120 [00:03<00:05, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42%|████▏     | 50/120 [00:03<00:04, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 52/120 [00:03<00:04, 14.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|████▌     | 54/120 [00:03<00:04, 14.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47%|████▋     | 56/120 [00:03<00:04, 14.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 58/120 [00:04<00:04, 14.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50%|█████     | 60/120 [00:04<00:04, 14.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|█████▏    | 62/120 [00:04<00:04, 14.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53%|█████▎    | 64/120 [00:04<00:03, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 66/120 [00:04<00:03, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 68/120 [00:04<00:03, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 70/120 [00:04<00:03, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 72/120 [00:05<00:03, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 74/120 [00:05<00:03, 14.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 76/120 [00:05<00:03, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 78/120 [00:05<00:02, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 80/120 [00:05<00:02, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 82/120 [00:05<00:02, 14.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 84/120 [00:05<00:02, 14.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 86/120 [00:06<00:02, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 88/120 [00:06<00:02, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 90/120 [00:06<00:02, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 92/120 [00:06<00:01, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 94/120 [00:06<00:01, 14.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 96/120 [00:06<00:01, 14.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 98/120 [00:06<00:01, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 100/120 [00:07<00:01, 14.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 102/120 [00:07<00:01, 14.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 104/120 [00:07<00:01, 14.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 106/120 [00:07<00:00, 14.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 108/120 [00:07<00:00, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 110/120 [00:07<00:00, 14.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 112/120 [00:07<00:00, 14.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 114/120 [00:08<00:00, 14.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 116/120 [00:08<00:00, 14.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 118/120 [00:08<00:00, 14.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 120/120 [00:08<00:00, 14.09it/s]\n",
            "08/05/2020 15:56:41 - INFO - __main__ -   ***** Eval results  *****\n",
            "08/05/2020 15:56:41 - INFO - __main__ -     eval_loss = 1.9478546837965647\n",
            "08/05/2020 15:56:41 - INFO - __main__ -     perplexity = tensor(7.0136)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "08/05/2020 15:56:41 - INFO - transformers.configuration_utils -   Configuration saved in ./textgenmodels/checkpoint-50/config.json\n",
            "08/05/2020 15:56:47 - INFO - transformers.modeling_utils -   Model weights saved in ./textgenmodels/checkpoint-50/pytorch_model.bin\n",
            "08/05/2020 15:56:47 - INFO - __main__ -   Saving model checkpoint to ./textgenmodels/checkpoint-50\n",
            "\n",
            "Iteration:  17%|█▋        | 20/120 [00:20<07:39,  4.59s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:20<05:28,  3.32s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:20<03:56,  2.41s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:21<02:51,  1.77s/it]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:21<02:07,  1.33s/it]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:21<01:36,  1.02s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:22<01:15,  1.25it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:22<01:00,  1.54it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:22<00:49,  1.85it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:22<00:42,  2.15it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:23<00:40,  2.22it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:23<00:36,  2.47it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:23<00:32,  2.71it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:24<00:29,  2.90it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:24<00:28,  3.05it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:24<00:26,  3.15it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:25<00:25,  3.25it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:25<00:25,  3.28it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:25<00:24,  3.28it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:25<00:24,  3.32it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:26<00:26,  2.97it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:26<00:25,  3.08it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:27<00:24,  3.15it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:27<00:23,  3.23it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:27<00:23,  3.30it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:27<00:22,  3.35it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:28<00:21,  3.40it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:28<00:21,  3.38it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:28<00:21,  3.41it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:29<00:20,  3.44it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:29<00:23,  3.04it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:29<00:21,  3.14it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:30<00:21,  3.20it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:30<00:20,  3.29it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:30<00:19,  3.34it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:30<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:31<00:18,  3.40it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:31<00:18,  3.43it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:31<00:18,  3.43it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:32<00:17,  3.42it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:32<00:19,  3.02it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:32<00:18,  3.12it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:33<00:18,  3.21it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:33<00:17,  3.28it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:33<00:16,  3.32it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:33<00:16,  3.36it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:34<00:15,  3.39it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:34<00:15,  3.42it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:34<00:15,  3.42it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:35<00:14,  3.42it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:35<00:16,  3.01it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:35<00:15,  3.11it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:36<00:15,  3.20it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:36<00:14,  3.28it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:36<00:13,  3.31it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:36<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:37<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:37<00:12,  3.40it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:37<00:12,  3.40it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:38<00:11,  3.42it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:38<00:13,  3.02it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:38<00:12,  3.09it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:39<00:11,  3.20it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:39<00:11,  3.25it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:39<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:40<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:40<00:10,  3.34it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:40<00:09,  3.35it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:40<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:41<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:41<00:09,  3.01it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:41<00:09,  3.11it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:42<00:08,  3.21it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:42<00:08,  3.29it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:42<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:43<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:43<00:07,  3.38it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:43<00:06,  3.39it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:44<00:07,  3.09it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:44<00:06,  3.17it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:44<00:06,  2.87it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:45<00:06,  2.98it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:45<00:05,  3.10it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:45<00:05,  3.17it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:45<00:04,  3.24it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:46<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:46<00:04,  3.32it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:46<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:47<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:47<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:47<00:03,  3.00it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:48<00:02,  3.07it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:48<00:02,  3.18it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:48<00:02,  3.25it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:49<00:01,  3.30it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:49<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:49<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:49<00:00,  3.37it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:50<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:50<00:00,  3.41it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:50<00:00,  2.36it/s]\n",
            "Epoch: 100%|██████████| 5/5 [03:17<00:00, 39.51s/it]\n",
            "08/05/2020 15:57:18 - INFO - __main__ -    global_step = 60, average loss = 3.3659872110933065\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58WmWwxiW5NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "3cba9e72-fa54-4f7a-95ff-48651aaea063"
      },
      "source": [
        "# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(parser, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = model_class.from_pretrained(args.output_dir)\n",
        "    tokenizer = YTEncoder.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2020 15:57:18 - INFO - __main__ -   Saving model checkpoint to ./textgenmodels\n",
            "08/05/2020 15:57:18 - INFO - transformers.configuration_utils -   Configuration saved in ./textgenmodels/config.json\n",
            "08/05/2020 15:57:26 - INFO - transformers.modeling_utils -   Model weights saved in ./textgenmodels/pytorch_model.bin\n",
            "08/05/2020 15:57:26 - INFO - transformers.configuration_utils -   loading configuration file ./textgenmodels/config.json\n",
            "08/05/2020 15:57:26 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"output_past\": true,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/05/2020 15:57:26 - INFO - transformers.modeling_utils -   loading weights file ./textgenmodels/pytorch_model.bin\n",
            "08/05/2020 15:57:40 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "08/05/2020 15:57:40 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./textgenmodels.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiU3UtCZjloQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a2d00e22-8ae8-4ca1-f540-3467862f33cf"
      },
      "source": [
        "CONTEXT_TEXT = ''\n",
        "\n",
        "START_TEXT = 'Ты чё '\n",
        "CONTEXT_TEXT = CONTEXT_TEXT + START_TEXT\n",
        "\n",
        "context_tokens = tokenizer.encode(CONTEXT_TEXT)\n",
        "sampled = sample_sequence(model, \n",
        "                          100, \n",
        "                          context_tokens, \n",
        "                          temperature = 1.0,\n",
        "                          top_p=0.80\n",
        "                          )\n",
        "\n",
        "out = sampled[:, len(context_tokens):].tolist()\n",
        "text = ''.join([tokenizer.decode(o) for o in out])\n",
        "\n",
        "# text = text[: text.find('<| endoftext|>')].split('\\n')\n",
        "text = text.split('\\n')\n",
        "\n",
        "print('-' * 20)\n",
        "\n",
        "for i, t in enumerate(text):\n",
        "    if i == 0:\n",
        "        print(START_TEXT + t)\n",
        "    else:\n",
        "        print(t)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 19.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Ты чё я — космополит\n",
            "На небе дым, под ним бетон\n",
            "Я поднимаюсь, как будто с горы\n",
            "Я лезу и лезу наверх\n",
            "Ты говоришь — «Он эгоцентрик»,\n",
            "Но мой эгоизм — это ты\n",
            "Ты хочешь сделать меня мышью? Я — пантера!\n",
            "<| endoftext|>\n",
            "Всё слишком идеализировано, слишком пристрастно\n",
            "До сих пор декадентство — это нонсенс\n",
            "А мы — дети репатрианта\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iekxNtc-qYkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! rm -rf ./textgenmodels/checkpoint-50\n",
        "# ! zip -r res_oxxxymiron.zip textgenmodels"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ29bAwsdekz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtXg1Bgtdk4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! cp ./res_oxxxymiron.zip './gdrive/My Drive/gpt2/res_oxxxymiron.zip'"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}